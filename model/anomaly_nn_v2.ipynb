{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.3"
    },
    "colab": {
      "name": "anomaly_nn.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "hbvFNaRk-yU2",
        "obJPMecX-yU6"
      ]
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "vJ7lKzXZ-yUl"
      },
      "source": [
        "import os\n",
        "import time\n",
        "import random\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# NOT SURE WHAT THE POINT OF THE CODE BELOW IS, REMOVE IF NOT NEEDED\n",
        "\n",
        "#if torch.cuda.is_available():\n",
        "#    torch.backends.cuda.deterministic = True"
      ],
      "execution_count": 95,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IEJlWELc-yUt"
      },
      "source": [
        "# For Mitch's error\n",
        "\n",
        "# import os\n",
        "# os.environ['KMP_DUPLICATE_LIB_OK']='True'"
      ],
      "execution_count": 96,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fvtKpraHC2nG",
        "outputId": "bd28c834-ef57-41ab-9e78-667a87e72f78"
      },
      "source": [
        "import os\n",
        "cwd = os.getcwd()\n",
        "print(cwd)"
      ],
      "execution_count": 97,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E162ja0yC3DR",
        "outputId": "8d4ace06-51f1-4071-9873-858de7d049bc"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 98,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wsxvHdJI-yUt"
      },
      "source": [
        "## Load and Prepare Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uoav9WT1-yUu"
      },
      "source": [
        "# # get data matrices and labels\n",
        "# train_data = np.load('../process/project_data/x_train_v1.npy')\n",
        "# read_train_labels = pd.read_csv('../process/project_data/y_train_v1.csv')\n",
        "# train_labels = read_train_labels['Label'] == 'Anomaly'\n",
        "# train_labels = train_labels.astype(int) # may not make sense to have an an int\n",
        "# test_data = np.load('../process/project_data/x_test_v1.npy')\n",
        "# read_test_labels = pd.read_csv('../process/project_data/y_test_v1.csv')\n",
        "# test_labels = read_test_labels['Label'] == 'Anomaly'\n",
        "# test_labels = test_labels.astype(int)\n",
        "\n",
        "\n",
        "data_loc = \"./drive/MyDrive/log_data/data_v5/\"\n",
        "data_version = \"v5\"\n",
        "\n",
        "# Train sets\n",
        "train_data = np.load('{}x_train_tf-idf_rolling_{}.npy'.format(data_loc, data_version))\n",
        "read_train_labels = pd.read_csv('{}y_train_tf-idf_rolling_{}.csv'.format(data_loc, data_version))\n",
        "train_labels = read_train_labels['Label'] == 'Anomaly'\n",
        "train_labels = train_labels.astype(int) # may not make sense to have an an int\n",
        "\n",
        "# Test sets\n",
        "test_data = np.load('{}x_test_tf-idf_rolling_{}.npy'.format(data_loc, data_version))\n",
        "read_test_labels = pd.read_csv('{}y_test_tf-idf_rolling_{}.csv'.format(data_loc, data_version))\n",
        "test_labels = read_test_labels['Label'] == 'Anomaly'\n",
        "test_labels = test_labels.astype(int)\n",
        "\n",
        "# data_dims = (19, 48)"
      ],
      "execution_count": 99,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "El0sG0q4CwjH",
        "outputId": "c383e179-206f-498a-806e-78b1a4b247f4"
      },
      "source": [
        "print(train_data.shape)\n",
        "print(train_labels.shape)"
      ],
      "execution_count": 100,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(394503, 16, 46)\n",
            "(394503,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fqTI7DFYCqf6",
        "outputId": "9cc5be24-273f-4d78-ab17-fb9cd64a9c03"
      },
      "source": [
        "print(test_data.shape)\n",
        "print(test_labels.shape)"
      ],
      "execution_count": 102,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(120537, 16, 46)\n",
            "(120537,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X09WrmMF-yUu"
      },
      "source": [
        "# Giving to do the below code, we need to add a index reset on y_train and y_val \n",
        "\n",
        "# # Create val set\n",
        "# random.seed(10)\n",
        "\n",
        "# val_percent_size = 0.10\n",
        "# val_size = int(train_data.shape[0] * val_percent_size)\n",
        "# val_idx = random.sample(range(0, train_data.shape[0]), val_size)\n",
        "\n",
        "# # select the val sets based on random indexing\n",
        "# val_data = np.take(train_data, val_idx, axis=0)\n",
        "# val_labels = train_labels.iloc[val_idx]\n",
        "\n",
        "# # delete the val indexes from the train data\n",
        "# train_data = np.delete(train_data, val_idx, axis=0)\n",
        "# train_labels = train_labels.drop(train_labels.index[[val_idx]])\n",
        "\n",
        "# # Val sets\n",
        "# val_data = np.load('{}x_val_tf-idf_rolling_v2.npy'.format(data_loc))\n",
        "# read_val_labels = pd.read_csv('{}y_train_tf-idf_rolling_v2.csv'.format(data_loc))\n",
        "# val_labels = read_val_labels['Label'] == 'Anomaly'\n",
        "# val_labels = val_labels.astype(int) # may not make sense to have an an int\n",
        "\n",
        "train_data, val_data, train_labels, val_labels = train_test_split(train_data, train_labels, test_size=0.2, random_state=42)\n"
      ],
      "execution_count": 103,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ta7khuA6F3e7",
        "outputId": "e6f2e703-5b5e-4aca-cd45-7078af6f3115"
      },
      "source": [
        "train_labels = train_labels.reset_index(drop=True)\n",
        "val_labels = val_labels.reset_index(drop=True)\n",
        "\n",
        "print(\"train data\")\n",
        "print(train_data.shape)\n",
        "print(train_labels.shape)\n",
        "print(\"val data\")\n",
        "print(val_data.shape)\n",
        "print(val_labels.shape)"
      ],
      "execution_count": 104,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "train data\n",
            "(315602, 16, 46)\n",
            "(315602,)\n",
            "val data\n",
            "(78901, 16, 46)\n",
            "(78901,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cD9rnBQo-yUv"
      },
      "source": [
        "# for testing on a subset\n",
        "# sub_set_size = 10000 * 3\n",
        "# train_data = train_data[0:sub_set_size]\n",
        "# test_data = test_data[0:sub_set_size]\n",
        "# val_data = val_data[0:sub_set_size]\n",
        "# val_labels = val_labels[0:sub_set_size]\n",
        "# train_labels = train_labels[0:sub_set_size]\n",
        "# test_labels = test_labels[0:sub_set_size]"
      ],
      "execution_count": 105,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fwQ39FNo-yUv",
        "outputId": "8b2957c2-1330-4554-af92-a7e7a4870f7f"
      },
      "source": [
        "data_dims = (train_data[1], train_data[2])\n",
        "print(train_data.shape)"
      ],
      "execution_count": 106,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(315602, 16, 46)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iSMHqrN8-yUx",
        "outputId": "4268e103-33e5-4a6b-f331-090faad4cec4"
      },
      "source": [
        "print(train_labels.shape)\n",
        "print(train_labels[0:5])\n",
        "print(type(train_labels))"
      ],
      "execution_count": 107,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(315602,)\n",
            "0    0\n",
            "1    1\n",
            "2    0\n",
            "3    0\n",
            "4    0\n",
            "Name: Label, dtype: int64\n",
            "<class 'pandas.core.series.Series'>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ydv5J-MW-yUx"
      },
      "source": [
        "# train_labels = pd.get_dummies(train_labels)\n",
        "# train_labels.values"
      ],
      "execution_count": 108,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sPraFUqn-yUx"
      },
      "source": [
        "class logDataset(Dataset):\n",
        "    \"\"\"Log Anomaly Features Dataset\"\"\"\n",
        "    \n",
        "    def __init__(self, data_vec, labels=None):\n",
        "        self.X = data_vec\n",
        "        self.y = labels\n",
        "        \n",
        "    def __len__(self):\n",
        "        return len(self.X)\n",
        "        \n",
        "    def __getitem__(self, idx):\n",
        "        data_matrix = self.X[idx]\n",
        "        \n",
        "        if not self.y is None:\n",
        "            return(data_matrix, self.y[idx])\n",
        "        else:\n",
        "            return data_matrix\n"
      ],
      "execution_count": 109,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aUdjevQL-yUy"
      },
      "source": [
        ""
      ],
      "execution_count": 109,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ol2-sxy5-yUy"
      },
      "source": [
        "# Convert to tensort and pad matrixes to have dims of 20x48 instead of 19x48\n",
        "# Also need to add a dimension for conv2D\n",
        "train_data = torch.tensor(train_data, dtype=torch.float32)\n",
        "train_data = F.pad(input=train_data, pad=(0, 0, 0, 1), mode='constant', value=0) # pad the bottom with 0s\n",
        "train_data = np.expand_dims(train_data, axis=1)\n",
        "test_data = torch.tensor(test_data, dtype=torch.float32)\n",
        "test_data = F.pad(input=test_data, pad=(0, 0, 0, 1), mode='constant', value=0)\n",
        "test_data = np.expand_dims(test_data, axis=1)\n",
        "val_data = torch.tensor(val_data, dtype=torch.float32)\n",
        "val_data = F.pad(input=val_data, pad=(0, 0, 0, 1), mode='constant', value=0)\n",
        "val_data = np.expand_dims(val_data, axis=1)"
      ],
      "execution_count": 110,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pK3Igl7j-yUz"
      },
      "source": [
        ""
      ],
      "execution_count": 110,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vm7eovFP-yUz",
        "outputId": "bcb7629b-8b60-453d-a2c8-b480d7ae0bc9"
      },
      "source": [
        "val_data.shape"
      ],
      "execution_count": 111,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(78901, 1, 17, 46)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 111
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ShW0RdNY-yU0"
      },
      "source": [
        "# Hyperparameters\n",
        "RANDOM_SEED = 1\n",
        "# THESE ARE ALL STARTING POINTS BASED ON LAB3\n",
        "LEARNING_RATE = 0.001\n",
        "BATCH_SIZE = 128\n",
        "NUM_EPOCHS = 10\n",
        "\n",
        "# Architecture\n",
        "# NUM_FEATURES = data_dims[0] * data_dims[1] # not sure this is needed currently\n",
        "\n",
        "# not sure we should be using an architecture with 2 classes with softmax\n",
        "# doesn't it make more sense to have a single output node\n",
        "# previous labs/lectures have 2 classes with softmax which is somewhat confusing\n",
        "NUM_CLASSES = 2\n",
        "\n",
        "# Other\n",
        "if torch.cuda.is_available():\n",
        "    DEVICE = \"cuda:0\"\n",
        "else:\n",
        "    DEVICE = \"cpu\"\n"
      ],
      "execution_count": 112,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I7_3tMRl-yU0"
      },
      "source": [
        "# pass datasets into the custom dataclass\n",
        "train_dataset = logDataset(train_data, labels = train_labels)\n",
        "test_dataset = logDataset(test_data, labels = test_labels)\n",
        "val_dataset = logDataset(val_data, labels = val_labels)"
      ],
      "execution_count": 113,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wYbeU4R2-yU1"
      },
      "source": [
        "# just a check that it worked\n",
        "# train_dataset.__getitem__(3)"
      ],
      "execution_count": 114,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zWwa2PG_-yU1",
        "outputId": "b1c93b4a-afa2-4aa6-bbcf-5d48227ce5e4"
      },
      "source": [
        "# use DataLoader class\n",
        "train_loader = DataLoader(dataset=train_dataset, \n",
        "                          batch_size=BATCH_SIZE, \n",
        "                          num_workers=0, # couldn't use workers https://github.com/fastai/fastbook/issues/85\n",
        "                          shuffle=True)\n",
        "\n",
        "test_loader = DataLoader(dataset=test_dataset, \n",
        "                         batch_size=BATCH_SIZE,\n",
        "                         num_workers=0,\n",
        "                         shuffle=False)\n",
        "\n",
        "val_loader = DataLoader(dataset=val_dataset, \n",
        "                         batch_size=BATCH_SIZE,\n",
        "                         num_workers=0,\n",
        "                         shuffle=False)\n",
        "\n",
        "\n",
        "# Checking the dataset\n",
        "for data, labels in train_loader:  \n",
        "    print('Matrix batch dimensions:', data.shape)\n",
        "    print('Matrix label dimensions:', labels.shape)\n",
        "    break\n",
        "\n",
        "# Checking the dataset\n",
        "for data, labels in test_loader:  \n",
        "    print('Matrix batch dimensions:', data.shape)\n",
        "    print('Matrix label dimensions:', labels.shape)\n",
        "    break\n",
        "    \n",
        "# Checking the dataset\n",
        "for data, labels in val_loader:  \n",
        "    print('Matrix batch dimensions:', data.shape)\n",
        "    print('Matrix label dimensions:', labels.shape)\n",
        "    break"
      ],
      "execution_count": 115,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Matrix batch dimensions: torch.Size([128, 1, 17, 46])\n",
            "Matrix label dimensions: torch.Size([128])\n",
            "Matrix batch dimensions: torch.Size([128, 1, 17, 46])\n",
            "Matrix label dimensions: torch.Size([128])\n",
            "Matrix batch dimensions: torch.Size([128, 1, 17, 46])\n",
            "Matrix label dimensions: torch.Size([128])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hbvFNaRk-yU2"
      },
      "source": [
        "## Setup Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KFxoCAHg-yU2",
        "outputId": "8bdd0c6d-2706-4269-a874-c97605b9371d"
      },
      "source": [
        "# this is something to do with warming up the nn\n",
        "# NEED TO FIGURE THIS OUT AND KEEP OR REMOVE\n",
        "device = torch.device(DEVICE)\n",
        "torch.manual_seed(0)\n",
        "\n",
        "for epoch in range(2):\n",
        "\n",
        "    for batch_idx, (x, y) in enumerate(train_loader):\n",
        "        \n",
        "        print('Epoch:', epoch+1, end='')\n",
        "        print(' | Batch index:', batch_idx, end='')\n",
        "        print(' | Batch size:', y.size()[0])\n",
        "        \n",
        "        x = x.to(device)\n",
        "        y = y.to(device)\n",
        "        break"
      ],
      "execution_count": 116,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 1 | Batch index: 0 | Batch size: 128\n",
            "Epoch: 2 | Batch index: 0 | Batch size: 128\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "doCsnPrG-yU2"
      },
      "source": [
        "##########################\n",
        "### MODEL\n",
        "##########################\n",
        "\n",
        "\n",
        "class logCNN(nn.Module):\n",
        "\n",
        "    def __init__(self, num_classes):\n",
        "        super(logCNN, self).__init__()\n",
        "        # noticed that in many of the example models we're initializing weights and biases but\n",
        "        # not in the CNN example, check into this\n",
        "        self.num_classes = num_classes\n",
        "        self.features = nn.Sequential(\n",
        "            \n",
        "            # Selection of 16 and 32 based on discussion with Debangsha, use for now\n",
        "            # will need to figure out kernel size that works for dims and potentially add pooling\n",
        "            # need to figure out how to deal with non-square image\n",
        "            # goes in as 20x48\n",
        "            nn.Conv2d(1, 16, kernel_size=2),\n",
        "            # comes out as 19x47\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2),\n",
        "            # comes out as ???\n",
        "            nn.Conv2d(16, 32, kernel_size=2), # can also add padding directly into here (but is equal on either side) - (1, 2) 1 on left and right and 2 each above and below\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2)\n",
        "        )\n",
        "\n",
        "        self.classifier = nn.Sequential(\n",
        "            # NEED TO FIGURE OUT THE SIZE OF THE IMAGE\n",
        "            nn.Linear(960, 120), # NEED TO DECIDE ON NUMBER OF NODES (120 BASED ON LAB3) ## NB changed to 1280 from 1408 based on error\n",
        "            nn.ReLU(),\n",
        "            # Maybe later try deleting below 2 lines to make the network shallower, hence reducing overfitting\n",
        "            nn.Linear(120, 84), # NEED TO DECIDE ON NUMBER OF NODES (84 BASED ON LAB3)\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(84, num_classes), # NOT SURE WE WANT TO BE USING 2 NODE OUTPUT\n",
        "        )\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.features(x)\n",
        "        x = torch.flatten(x, 1)\n",
        "#         print(x.shape) # Was printing in the training logging\n",
        "        logits = self.classifier(x)\n",
        "        probas = F.softmax(logits, dim=1)\n",
        "        return logits, probas\n"
      ],
      "execution_count": 117,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EZLIAStI-yU3"
      },
      "source": [
        "torch.manual_seed(RANDOM_SEED)\n",
        "\n",
        "model = logCNN(NUM_CLASSES) # THIS WILL BE UPDATED IF NOT USING 2 OUTPUT NODES\n",
        "model.to(DEVICE)\n",
        "\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)"
      ],
      "execution_count": 118,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dr-b4WVw-yU3"
      },
      "source": [
        "# Train Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": [
          "outputPrepend"
        ],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SeNobFr9-yU4",
        "outputId": "191d2ce0-8206-43ce-b153-015db1d17748"
      },
      "source": [
        "# NEED TO FIGURE OUT THE BEST FUNCTION FOR ACCURACY\n",
        "# NEED TO FOCUS ON TRUE/FALSE, NOT ON CLASSES\n",
        "def compute_accuracy(model, data_loader, device):\n",
        "    correct_pred, num_examples = 0, 0\n",
        "    for i, (features, targets) in enumerate(data_loader):\n",
        "            \n",
        "        features = features.to(DEVICE)\n",
        "        \n",
        "        # NOTE TO SELF WHEN CHANGING INT TARGETS AT TOP OF NOTEBOOK, WILL ALSO HAVE TO CHANGE / REMOVE .long here\n",
        "        targets = targets.to(DEVICE, dtype=torch.long) # had to pass in to torch.long based on the 1, 0 int labels\n",
        "\n",
        "        logits, probas = model(features)\n",
        "        _, predicted_labels = torch.max(probas, 1)\n",
        "        num_examples += targets.size(0)\n",
        "        correct_pred += (predicted_labels == targets).sum()\n",
        "    return correct_pred.float()/num_examples * 100\n",
        "\n",
        "# This needs to be vectorized\n",
        "def compute_f1(model, data_loader, device):\n",
        "    y_hats = []\n",
        "    y_acts = []\n",
        "    for i, (inputs, targets) in enumerate(data_loader):\n",
        "        yhat = model(inputs)[-1].cpu().detach().numpy().round()\n",
        "        yhat = np.argmax(yhat, axis=1)\n",
        "        y_hats.append(yhat)\n",
        "        y_acts.append(list(targets.cpu().detach().numpy()))\n",
        "    \n",
        "    y_hats = [item for sublist in y_hats for item in sublist]\n",
        "    y_acts = [item for sublist in y_acts for item in sublist]\n",
        "    return f1_score(y_acts, y_hats)\n",
        "\n",
        "  \n",
        "\n",
        "start_time = time.time()\n",
        "minibatch_cost = [] # this isn't in the CNN model example but is from lecture 3 example files\n",
        "epoch_train_performance = []\n",
        "epoch_val_performance = []\n",
        "\n",
        "# criterion = nn.BCEWithLogitsLoss()\n",
        "\n",
        "for epoch in range(NUM_EPOCHS):\n",
        "    \n",
        "    model.train()\n",
        "    for batch_idx, (features, targets) in enumerate(train_loader):\n",
        "        \n",
        "        features = features.to(DEVICE)\n",
        "        targets = targets.to(DEVICE, dtype=torch.long) # another had to use torch.long\n",
        "            \n",
        "        ### FORWARD AND BACK PROP\n",
        "        logits, probas = model(features)\n",
        "        \n",
        "        # try binary CE\n",
        "        cost = F.cross_entropy(logits, targets) # NEED TO MODIFY BASED ON BEST COST FUNCTION\n",
        "\n",
        "#         loss = criterion(logits, targets.unsqueeze(1))\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        \n",
        "        cost.backward()\n",
        "#         loss.backward()\n",
        "        minibatch_cost.append(cost) # from lecture 3 example files\n",
        "        \n",
        "        ### UPDATE MODEL PARAMETERS\n",
        "        optimizer.step()\n",
        "        \n",
        "        ### LOGGING - MAY WANT TO MODIFY LOGGING INTERVALS\n",
        "        if not batch_idx % 50:\n",
        "            print ('Epoch: %03d/%03d | Batch %04d/%04d | Cost: %.4f' \n",
        "                   %(epoch+1, NUM_EPOCHS, batch_idx, \n",
        "                     len(train_loader), cost, ))\n",
        "\n",
        "        \n",
        "    model.eval()\n",
        "    with torch.set_grad_enabled(False): # save memory during inference\n",
        "        train_performance = compute_f1(model, train_loader, device=DEVICE)\n",
        "        val_performance = compute_f1(model, val_loader, device=DEVICE)\n",
        "        epoch_train_performance.append(train_performance) # from lecture 3 example files\n",
        "        epoch_val_performance.append(val_performance)\n",
        "        print('Epoch: %03d/%03d | Train: %.3f%%   | Val: %.3f%%' % (\n",
        "              epoch+1, NUM_EPOCHS, train_performance, val_performance))\n",
        "        \n",
        "    print('Time elapsed: %.2f min' % ((time.time() - start_time)/60))\n",
        "    \n",
        "print('Total Training Time: %.2f min' % ((time.time() - start_time)/60))"
      ],
      "execution_count": 119,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 001/010 | Batch 0000/2466 | Cost: 0.7804\n",
            "Epoch: 001/010 | Batch 0050/2466 | Cost: 0.1301\n",
            "Epoch: 001/010 | Batch 0100/2466 | Cost: 0.0067\n",
            "Epoch: 001/010 | Batch 0150/2466 | Cost: 0.0214\n",
            "Epoch: 001/010 | Batch 0200/2466 | Cost: 0.0234\n",
            "Epoch: 001/010 | Batch 0250/2466 | Cost: 0.0020\n",
            "Epoch: 001/010 | Batch 0300/2466 | Cost: 0.0044\n",
            "Epoch: 001/010 | Batch 0350/2466 | Cost: 0.0049\n",
            "Epoch: 001/010 | Batch 0400/2466 | Cost: 0.0019\n",
            "Epoch: 001/010 | Batch 0450/2466 | Cost: 0.0006\n",
            "Epoch: 001/010 | Batch 0500/2466 | Cost: 0.0002\n",
            "Epoch: 001/010 | Batch 0550/2466 | Cost: 0.0007\n",
            "Epoch: 001/010 | Batch 0600/2466 | Cost: 0.0357\n",
            "Epoch: 001/010 | Batch 0650/2466 | Cost: 0.0001\n",
            "Epoch: 001/010 | Batch 0700/2466 | Cost: 0.0009\n",
            "Epoch: 001/010 | Batch 0750/2466 | Cost: 0.0001\n",
            "Epoch: 001/010 | Batch 0800/2466 | Cost: 0.0093\n",
            "Epoch: 001/010 | Batch 0850/2466 | Cost: 0.0025\n",
            "Epoch: 001/010 | Batch 0900/2466 | Cost: 0.0001\n",
            "Epoch: 001/010 | Batch 0950/2466 | Cost: 0.0000\n",
            "Epoch: 001/010 | Batch 1000/2466 | Cost: 0.0002\n",
            "Epoch: 001/010 | Batch 1050/2466 | Cost: 0.0001\n",
            "Epoch: 001/010 | Batch 1100/2466 | Cost: 0.0124\n",
            "Epoch: 001/010 | Batch 1150/2466 | Cost: 0.0084\n",
            "Epoch: 001/010 | Batch 1200/2466 | Cost: 0.0015\n",
            "Epoch: 001/010 | Batch 1250/2466 | Cost: 0.0007\n",
            "Epoch: 001/010 | Batch 1300/2466 | Cost: 0.0000\n",
            "Epoch: 001/010 | Batch 1350/2466 | Cost: 0.0007\n",
            "Epoch: 001/010 | Batch 1400/2466 | Cost: 0.0001\n",
            "Epoch: 001/010 | Batch 1450/2466 | Cost: 0.0009\n",
            "Epoch: 001/010 | Batch 1500/2466 | Cost: 0.0000\n",
            "Epoch: 001/010 | Batch 1550/2466 | Cost: 0.0010\n",
            "Epoch: 001/010 | Batch 1600/2466 | Cost: 0.0027\n",
            "Epoch: 001/010 | Batch 1650/2466 | Cost: 0.0005\n",
            "Epoch: 001/010 | Batch 1700/2466 | Cost: 0.0005\n",
            "Epoch: 001/010 | Batch 1750/2466 | Cost: 0.0000\n",
            "Epoch: 001/010 | Batch 1800/2466 | Cost: 0.0021\n",
            "Epoch: 001/010 | Batch 1850/2466 | Cost: 0.0069\n",
            "Epoch: 001/010 | Batch 1900/2466 | Cost: 0.0018\n",
            "Epoch: 001/010 | Batch 1950/2466 | Cost: 0.0015\n",
            "Epoch: 001/010 | Batch 2000/2466 | Cost: 0.0001\n",
            "Epoch: 001/010 | Batch 2050/2466 | Cost: 0.0000\n",
            "Epoch: 001/010 | Batch 2100/2466 | Cost: 0.0002\n",
            "Epoch: 001/010 | Batch 2150/2466 | Cost: 0.0000\n",
            "Epoch: 001/010 | Batch 2200/2466 | Cost: 0.0345\n",
            "Epoch: 001/010 | Batch 2250/2466 | Cost: 0.0001\n",
            "Epoch: 001/010 | Batch 2300/2466 | Cost: 0.0186\n",
            "Epoch: 001/010 | Batch 2350/2466 | Cost: 0.0023\n",
            "Epoch: 001/010 | Batch 2400/2466 | Cost: 0.0001\n",
            "Epoch: 001/010 | Batch 2450/2466 | Cost: 0.0001\n",
            "Epoch: 001/010 | Train: 0.991%   | Val: 0.991%\n",
            "Time elapsed: 3.12 min\n",
            "Epoch: 002/010 | Batch 0000/2466 | Cost: 0.0000\n",
            "Epoch: 002/010 | Batch 0050/2466 | Cost: 0.0000\n",
            "Epoch: 002/010 | Batch 0100/2466 | Cost: 0.0003\n",
            "Epoch: 002/010 | Batch 0150/2466 | Cost: 0.0001\n",
            "Epoch: 002/010 | Batch 0200/2466 | Cost: 0.0007\n",
            "Epoch: 002/010 | Batch 0250/2466 | Cost: 0.0000\n",
            "Epoch: 002/010 | Batch 0300/2466 | Cost: 0.0011\n",
            "Epoch: 002/010 | Batch 0350/2466 | Cost: 0.0007\n",
            "Epoch: 002/010 | Batch 0400/2466 | Cost: 0.0004\n",
            "Epoch: 002/010 | Batch 0450/2466 | Cost: 0.0000\n",
            "Epoch: 002/010 | Batch 0500/2466 | Cost: 0.0000\n",
            "Epoch: 002/010 | Batch 0550/2466 | Cost: 0.0000\n",
            "Epoch: 002/010 | Batch 0600/2466 | Cost: 0.0001\n",
            "Epoch: 002/010 | Batch 0650/2466 | Cost: 0.0001\n",
            "Epoch: 002/010 | Batch 0700/2466 | Cost: 0.0000\n",
            "Epoch: 002/010 | Batch 0750/2466 | Cost: 0.0001\n",
            "Epoch: 002/010 | Batch 0800/2466 | Cost: 0.0002\n",
            "Epoch: 002/010 | Batch 0850/2466 | Cost: 0.0000\n",
            "Epoch: 002/010 | Batch 0900/2466 | Cost: 0.0001\n",
            "Epoch: 002/010 | Batch 0950/2466 | Cost: 0.0001\n",
            "Epoch: 002/010 | Batch 1000/2466 | Cost: 0.0009\n",
            "Epoch: 002/010 | Batch 1050/2466 | Cost: 0.0002\n",
            "Epoch: 002/010 | Batch 1100/2466 | Cost: 0.0001\n",
            "Epoch: 002/010 | Batch 1150/2466 | Cost: 0.0014\n",
            "Epoch: 002/010 | Batch 1200/2466 | Cost: 0.0043\n",
            "Epoch: 002/010 | Batch 1250/2466 | Cost: 0.0002\n",
            "Epoch: 002/010 | Batch 1300/2466 | Cost: 0.0001\n",
            "Epoch: 002/010 | Batch 1350/2466 | Cost: 0.0000\n",
            "Epoch: 002/010 | Batch 1400/2466 | Cost: 0.0002\n",
            "Epoch: 002/010 | Batch 1450/2466 | Cost: 0.0000\n",
            "Epoch: 002/010 | Batch 1500/2466 | Cost: 0.0000\n",
            "Epoch: 002/010 | Batch 1550/2466 | Cost: 0.0000\n",
            "Epoch: 002/010 | Batch 1600/2466 | Cost: 0.0015\n",
            "Epoch: 002/010 | Batch 1650/2466 | Cost: 0.0000\n",
            "Epoch: 002/010 | Batch 1700/2466 | Cost: 0.0001\n",
            "Epoch: 002/010 | Batch 1750/2466 | Cost: 0.0012\n",
            "Epoch: 002/010 | Batch 1800/2466 | Cost: 0.0000\n",
            "Epoch: 002/010 | Batch 1850/2466 | Cost: 0.0012\n",
            "Epoch: 002/010 | Batch 1900/2466 | Cost: 0.0000\n",
            "Epoch: 002/010 | Batch 1950/2466 | Cost: 0.0010\n",
            "Epoch: 002/010 | Batch 2000/2466 | Cost: 0.0000\n",
            "Epoch: 002/010 | Batch 2050/2466 | Cost: 0.0000\n",
            "Epoch: 002/010 | Batch 2100/2466 | Cost: 0.0019\n",
            "Epoch: 002/010 | Batch 2150/2466 | Cost: 0.0000\n",
            "Epoch: 002/010 | Batch 2200/2466 | Cost: 0.0001\n",
            "Epoch: 002/010 | Batch 2250/2466 | Cost: 0.0001\n",
            "Epoch: 002/010 | Batch 2300/2466 | Cost: 0.0000\n",
            "Epoch: 002/010 | Batch 2350/2466 | Cost: 0.0007\n",
            "Epoch: 002/010 | Batch 2400/2466 | Cost: 0.0001\n",
            "Epoch: 002/010 | Batch 2450/2466 | Cost: 0.0002\n",
            "Epoch: 002/010 | Train: 0.994%   | Val: 0.994%\n",
            "Time elapsed: 6.24 min\n",
            "Epoch: 003/010 | Batch 0000/2466 | Cost: 0.0000\n",
            "Epoch: 003/010 | Batch 0050/2466 | Cost: 0.0000\n",
            "Epoch: 003/010 | Batch 0100/2466 | Cost: 0.0000\n",
            "Epoch: 003/010 | Batch 0150/2466 | Cost: 0.0000\n",
            "Epoch: 003/010 | Batch 0200/2466 | Cost: 0.0003\n",
            "Epoch: 003/010 | Batch 0250/2466 | Cost: 0.0019\n",
            "Epoch: 003/010 | Batch 0300/2466 | Cost: 0.0001\n",
            "Epoch: 003/010 | Batch 0350/2466 | Cost: 0.0001\n",
            "Epoch: 003/010 | Batch 0400/2466 | Cost: 0.0000\n",
            "Epoch: 003/010 | Batch 0450/2466 | Cost: 0.0003\n",
            "Epoch: 003/010 | Batch 0500/2466 | Cost: 0.0003\n",
            "Epoch: 003/010 | Batch 0550/2466 | Cost: 0.0005\n",
            "Epoch: 003/010 | Batch 0600/2466 | Cost: 0.0018\n",
            "Epoch: 003/010 | Batch 0650/2466 | Cost: 0.0008\n",
            "Epoch: 003/010 | Batch 0700/2466 | Cost: 0.0000\n",
            "Epoch: 003/010 | Batch 0750/2466 | Cost: 0.0000\n",
            "Epoch: 003/010 | Batch 0800/2466 | Cost: 0.0000\n",
            "Epoch: 003/010 | Batch 0850/2466 | Cost: 0.0004\n",
            "Epoch: 003/010 | Batch 0900/2466 | Cost: 0.0001\n",
            "Epoch: 003/010 | Batch 0950/2466 | Cost: 0.0003\n",
            "Epoch: 003/010 | Batch 1000/2466 | Cost: 0.0000\n",
            "Epoch: 003/010 | Batch 1050/2466 | Cost: 0.0037\n",
            "Epoch: 003/010 | Batch 1100/2466 | Cost: 0.0000\n",
            "Epoch: 003/010 | Batch 1150/2466 | Cost: 0.0001\n",
            "Epoch: 003/010 | Batch 1200/2466 | Cost: 0.0000\n",
            "Epoch: 003/010 | Batch 1250/2466 | Cost: 0.0000\n",
            "Epoch: 003/010 | Batch 1300/2466 | Cost: 0.0001\n",
            "Epoch: 003/010 | Batch 1350/2466 | Cost: 0.0162\n",
            "Epoch: 003/010 | Batch 1400/2466 | Cost: 0.0000\n",
            "Epoch: 003/010 | Batch 1450/2466 | Cost: 0.0000\n",
            "Epoch: 003/010 | Batch 1500/2466 | Cost: 0.0013\n",
            "Epoch: 003/010 | Batch 1550/2466 | Cost: 0.0000\n",
            "Epoch: 003/010 | Batch 1600/2466 | Cost: 0.0000\n",
            "Epoch: 003/010 | Batch 1650/2466 | Cost: 0.0002\n",
            "Epoch: 003/010 | Batch 1700/2466 | Cost: 0.0004\n",
            "Epoch: 003/010 | Batch 1750/2466 | Cost: 0.0001\n",
            "Epoch: 003/010 | Batch 1800/2466 | Cost: 0.0003\n",
            "Epoch: 003/010 | Batch 1850/2466 | Cost: 0.0004\n",
            "Epoch: 003/010 | Batch 1900/2466 | Cost: 0.0000\n",
            "Epoch: 003/010 | Batch 1950/2466 | Cost: 0.0000\n",
            "Epoch: 003/010 | Batch 2000/2466 | Cost: 0.0000\n",
            "Epoch: 003/010 | Batch 2050/2466 | Cost: 0.0006\n",
            "Epoch: 003/010 | Batch 2100/2466 | Cost: 0.0000\n",
            "Epoch: 003/010 | Batch 2150/2466 | Cost: 0.0000\n",
            "Epoch: 003/010 | Batch 2200/2466 | Cost: 0.0000\n",
            "Epoch: 003/010 | Batch 2250/2466 | Cost: 0.0000\n",
            "Epoch: 003/010 | Batch 2300/2466 | Cost: 0.0000\n",
            "Epoch: 003/010 | Batch 2350/2466 | Cost: 0.0019\n",
            "Epoch: 003/010 | Batch 2400/2466 | Cost: 0.0152\n",
            "Epoch: 003/010 | Batch 2450/2466 | Cost: 0.0002\n",
            "Epoch: 003/010 | Train: 0.991%   | Val: 0.993%\n",
            "Time elapsed: 9.36 min\n",
            "Epoch: 004/010 | Batch 0000/2466 | Cost: 0.0000\n",
            "Epoch: 004/010 | Batch 0050/2466 | Cost: 0.0005\n",
            "Epoch: 004/010 | Batch 0100/2466 | Cost: 0.0001\n",
            "Epoch: 004/010 | Batch 0150/2466 | Cost: 0.0013\n",
            "Epoch: 004/010 | Batch 0200/2466 | Cost: 0.0001\n",
            "Epoch: 004/010 | Batch 0250/2466 | Cost: 0.0174\n",
            "Epoch: 004/010 | Batch 0300/2466 | Cost: 0.0000\n",
            "Epoch: 004/010 | Batch 0350/2466 | Cost: 0.0000\n",
            "Epoch: 004/010 | Batch 0400/2466 | Cost: 0.0005\n",
            "Epoch: 004/010 | Batch 0450/2466 | Cost: 0.0009\n",
            "Epoch: 004/010 | Batch 0500/2466 | Cost: 0.0006\n",
            "Epoch: 004/010 | Batch 0550/2466 | Cost: 0.0001\n",
            "Epoch: 004/010 | Batch 0600/2466 | Cost: 0.0001\n",
            "Epoch: 004/010 | Batch 0650/2466 | Cost: 0.0000\n",
            "Epoch: 004/010 | Batch 0700/2466 | Cost: 0.0003\n",
            "Epoch: 004/010 | Batch 0750/2466 | Cost: 0.0086\n",
            "Epoch: 004/010 | Batch 0800/2466 | Cost: 0.0003\n",
            "Epoch: 004/010 | Batch 0850/2466 | Cost: 0.0013\n",
            "Epoch: 004/010 | Batch 0900/2466 | Cost: 0.0000\n",
            "Epoch: 004/010 | Batch 0950/2466 | Cost: 0.0073\n",
            "Epoch: 004/010 | Batch 1000/2466 | Cost: 0.0001\n",
            "Epoch: 004/010 | Batch 1050/2466 | Cost: 0.0004\n",
            "Epoch: 004/010 | Batch 1100/2466 | Cost: 0.0001\n",
            "Epoch: 004/010 | Batch 1150/2466 | Cost: 0.0013\n",
            "Epoch: 004/010 | Batch 1200/2466 | Cost: 0.0000\n",
            "Epoch: 004/010 | Batch 1250/2466 | Cost: 0.0000\n",
            "Epoch: 004/010 | Batch 1300/2466 | Cost: 0.0000\n",
            "Epoch: 004/010 | Batch 1350/2466 | Cost: 0.0000\n",
            "Epoch: 004/010 | Batch 1400/2466 | Cost: 0.0000\n",
            "Epoch: 004/010 | Batch 1450/2466 | Cost: 0.0003\n",
            "Epoch: 004/010 | Batch 1500/2466 | Cost: 0.0000\n",
            "Epoch: 004/010 | Batch 1550/2466 | Cost: 0.0000\n",
            "Epoch: 004/010 | Batch 1600/2466 | Cost: 0.0000\n",
            "Epoch: 004/010 | Batch 1650/2466 | Cost: 0.0000\n",
            "Epoch: 004/010 | Batch 1700/2466 | Cost: 0.0000\n",
            "Epoch: 004/010 | Batch 1750/2466 | Cost: 0.0008\n",
            "Epoch: 004/010 | Batch 1800/2466 | Cost: 0.0047\n",
            "Epoch: 004/010 | Batch 1850/2466 | Cost: 0.0000\n",
            "Epoch: 004/010 | Batch 1900/2466 | Cost: 0.0007\n",
            "Epoch: 004/010 | Batch 1950/2466 | Cost: 0.0000\n",
            "Epoch: 004/010 | Batch 2000/2466 | Cost: 0.0000\n",
            "Epoch: 004/010 | Batch 2050/2466 | Cost: 0.0001\n",
            "Epoch: 004/010 | Batch 2100/2466 | Cost: 0.0005\n",
            "Epoch: 004/010 | Batch 2150/2466 | Cost: 0.0001\n",
            "Epoch: 004/010 | Batch 2200/2466 | Cost: 0.0000\n",
            "Epoch: 004/010 | Batch 2250/2466 | Cost: 0.0000\n",
            "Epoch: 004/010 | Batch 2300/2466 | Cost: 0.0003\n",
            "Epoch: 004/010 | Batch 2350/2466 | Cost: 0.0000\n",
            "Epoch: 004/010 | Batch 2400/2466 | Cost: 0.0000\n",
            "Epoch: 004/010 | Batch 2450/2466 | Cost: 0.0006\n",
            "Epoch: 004/010 | Train: 0.995%   | Val: 0.997%\n",
            "Time elapsed: 12.51 min\n",
            "Epoch: 005/010 | Batch 0000/2466 | Cost: 0.0000\n",
            "Epoch: 005/010 | Batch 0050/2466 | Cost: 0.0000\n",
            "Epoch: 005/010 | Batch 0100/2466 | Cost: 0.0000\n",
            "Epoch: 005/010 | Batch 0150/2466 | Cost: 0.0000\n",
            "Epoch: 005/010 | Batch 0200/2466 | Cost: 0.0004\n",
            "Epoch: 005/010 | Batch 0250/2466 | Cost: 0.0012\n",
            "Epoch: 005/010 | Batch 0300/2466 | Cost: 0.0004\n",
            "Epoch: 005/010 | Batch 0350/2466 | Cost: 0.0037\n",
            "Epoch: 005/010 | Batch 0400/2466 | Cost: 0.0000\n",
            "Epoch: 005/010 | Batch 0450/2466 | Cost: 0.0000\n",
            "Epoch: 005/010 | Batch 0500/2466 | Cost: 0.0001\n",
            "Epoch: 005/010 | Batch 0550/2466 | Cost: 0.0000\n",
            "Epoch: 005/010 | Batch 0600/2466 | Cost: 0.0000\n",
            "Epoch: 005/010 | Batch 0650/2466 | Cost: 0.0000\n",
            "Epoch: 005/010 | Batch 0700/2466 | Cost: 0.0000\n",
            "Epoch: 005/010 | Batch 0750/2466 | Cost: 0.0008\n",
            "Epoch: 005/010 | Batch 0800/2466 | Cost: 0.0000\n",
            "Epoch: 005/010 | Batch 0850/2466 | Cost: 0.0000\n",
            "Epoch: 005/010 | Batch 0900/2466 | Cost: 0.0000\n",
            "Epoch: 005/010 | Batch 0950/2466 | Cost: 0.0002\n",
            "Epoch: 005/010 | Batch 1000/2466 | Cost: 0.0000\n",
            "Epoch: 005/010 | Batch 1050/2466 | Cost: 0.0001\n",
            "Epoch: 005/010 | Batch 1100/2466 | Cost: 0.0003\n",
            "Epoch: 005/010 | Batch 1150/2466 | Cost: 0.0004\n",
            "Epoch: 005/010 | Batch 1200/2466 | Cost: 0.0000\n",
            "Epoch: 005/010 | Batch 1250/2466 | Cost: 0.0002\n",
            "Epoch: 005/010 | Batch 1300/2466 | Cost: 0.0716\n",
            "Epoch: 005/010 | Batch 1350/2466 | Cost: 0.0001\n",
            "Epoch: 005/010 | Batch 1400/2466 | Cost: 0.0001\n",
            "Epoch: 005/010 | Batch 1450/2466 | Cost: 0.0008\n",
            "Epoch: 005/010 | Batch 1500/2466 | Cost: 0.0002\n",
            "Epoch: 005/010 | Batch 1550/2466 | Cost: 0.0001\n",
            "Epoch: 005/010 | Batch 1600/2466 | Cost: 0.0000\n",
            "Epoch: 005/010 | Batch 1650/2466 | Cost: 0.0002\n",
            "Epoch: 005/010 | Batch 1700/2466 | Cost: 0.0001\n",
            "Epoch: 005/010 | Batch 1750/2466 | Cost: 0.0000\n",
            "Epoch: 005/010 | Batch 1800/2466 | Cost: 0.0002\n",
            "Epoch: 005/010 | Batch 1850/2466 | Cost: 0.0000\n",
            "Epoch: 005/010 | Batch 1900/2466 | Cost: 0.0003\n",
            "Epoch: 005/010 | Batch 1950/2466 | Cost: 0.0001\n",
            "Epoch: 005/010 | Batch 2000/2466 | Cost: 0.0012\n",
            "Epoch: 005/010 | Batch 2050/2466 | Cost: 0.0001\n",
            "Epoch: 005/010 | Batch 2100/2466 | Cost: 0.0000\n",
            "Epoch: 005/010 | Batch 2150/2466 | Cost: 0.0002\n",
            "Epoch: 005/010 | Batch 2200/2466 | Cost: 0.0000\n",
            "Epoch: 005/010 | Batch 2250/2466 | Cost: 0.0005\n",
            "Epoch: 005/010 | Batch 2300/2466 | Cost: 0.0000\n",
            "Epoch: 005/010 | Batch 2350/2466 | Cost: 0.0005\n",
            "Epoch: 005/010 | Batch 2400/2466 | Cost: 0.0001\n",
            "Epoch: 005/010 | Batch 2450/2466 | Cost: 0.0004\n",
            "Epoch: 005/010 | Train: 0.994%   | Val: 0.996%\n",
            "Time elapsed: 15.87 min\n",
            "Epoch: 006/010 | Batch 0000/2466 | Cost: 0.0000\n",
            "Epoch: 006/010 | Batch 0050/2466 | Cost: 0.0000\n",
            "Epoch: 006/010 | Batch 0100/2466 | Cost: 0.0000\n",
            "Epoch: 006/010 | Batch 0150/2466 | Cost: 0.0003\n",
            "Epoch: 006/010 | Batch 0200/2466 | Cost: 0.0002\n",
            "Epoch: 006/010 | Batch 0250/2466 | Cost: 0.0000\n",
            "Epoch: 006/010 | Batch 0300/2466 | Cost: 0.0000\n",
            "Epoch: 006/010 | Batch 0350/2466 | Cost: 0.0000\n",
            "Epoch: 006/010 | Batch 0400/2466 | Cost: 0.0011\n",
            "Epoch: 006/010 | Batch 0450/2466 | Cost: 0.0000\n",
            "Epoch: 006/010 | Batch 0500/2466 | Cost: 0.0001\n",
            "Epoch: 006/010 | Batch 0550/2466 | Cost: 0.0004\n",
            "Epoch: 006/010 | Batch 0600/2466 | Cost: 0.0012\n",
            "Epoch: 006/010 | Batch 0650/2466 | Cost: 0.0000\n",
            "Epoch: 006/010 | Batch 0700/2466 | Cost: 0.0000\n",
            "Epoch: 006/010 | Batch 0750/2466 | Cost: 0.0000\n",
            "Epoch: 006/010 | Batch 0800/2466 | Cost: 0.0001\n",
            "Epoch: 006/010 | Batch 0850/2466 | Cost: 0.0001\n",
            "Epoch: 006/010 | Batch 0900/2466 | Cost: 0.0001\n",
            "Epoch: 006/010 | Batch 0950/2466 | Cost: 0.0002\n",
            "Epoch: 006/010 | Batch 1000/2466 | Cost: 0.0001\n",
            "Epoch: 006/010 | Batch 1050/2466 | Cost: 0.0019\n",
            "Epoch: 006/010 | Batch 1100/2466 | Cost: 0.0000\n",
            "Epoch: 006/010 | Batch 1150/2466 | Cost: 0.0000\n",
            "Epoch: 006/010 | Batch 1200/2466 | Cost: 0.0001\n",
            "Epoch: 006/010 | Batch 1250/2466 | Cost: 0.0337\n",
            "Epoch: 006/010 | Batch 1300/2466 | Cost: 0.0007\n",
            "Epoch: 006/010 | Batch 1350/2466 | Cost: 0.0000\n",
            "Epoch: 006/010 | Batch 1400/2466 | Cost: 0.0000\n",
            "Epoch: 006/010 | Batch 1450/2466 | Cost: 0.0001\n",
            "Epoch: 006/010 | Batch 1500/2466 | Cost: 0.0000\n",
            "Epoch: 006/010 | Batch 1550/2466 | Cost: 0.0000\n",
            "Epoch: 006/010 | Batch 1600/2466 | Cost: 0.0003\n",
            "Epoch: 006/010 | Batch 1650/2466 | Cost: 0.0004\n",
            "Epoch: 006/010 | Batch 1700/2466 | Cost: 0.0000\n",
            "Epoch: 006/010 | Batch 1750/2466 | Cost: 0.0004\n",
            "Epoch: 006/010 | Batch 1800/2466 | Cost: 0.0001\n",
            "Epoch: 006/010 | Batch 1850/2466 | Cost: 0.0017\n",
            "Epoch: 006/010 | Batch 1900/2466 | Cost: 0.0000\n",
            "Epoch: 006/010 | Batch 1950/2466 | Cost: 0.0001\n",
            "Epoch: 006/010 | Batch 2000/2466 | Cost: 0.0000\n",
            "Epoch: 006/010 | Batch 2050/2466 | Cost: 0.0001\n",
            "Epoch: 006/010 | Batch 2100/2466 | Cost: 0.0000\n",
            "Epoch: 006/010 | Batch 2150/2466 | Cost: 0.0001\n",
            "Epoch: 006/010 | Batch 2200/2466 | Cost: 0.0000\n",
            "Epoch: 006/010 | Batch 2250/2466 | Cost: 0.0007\n",
            "Epoch: 006/010 | Batch 2300/2466 | Cost: 0.0000\n",
            "Epoch: 006/010 | Batch 2350/2466 | Cost: 0.0000\n",
            "Epoch: 006/010 | Batch 2400/2466 | Cost: 0.0384\n",
            "Epoch: 006/010 | Batch 2450/2466 | Cost: 0.0003\n",
            "Epoch: 006/010 | Train: 0.994%   | Val: 0.994%\n",
            "Time elapsed: 19.16 min\n",
            "Epoch: 007/010 | Batch 0000/2466 | Cost: 0.0001\n",
            "Epoch: 007/010 | Batch 0050/2466 | Cost: 0.0000\n",
            "Epoch: 007/010 | Batch 0100/2466 | Cost: 0.0000\n",
            "Epoch: 007/010 | Batch 0150/2466 | Cost: 0.0001\n",
            "Epoch: 007/010 | Batch 0200/2466 | Cost: 0.0000\n",
            "Epoch: 007/010 | Batch 0250/2466 | Cost: 0.0000\n",
            "Epoch: 007/010 | Batch 0300/2466 | Cost: 0.0002\n",
            "Epoch: 007/010 | Batch 0350/2466 | Cost: 0.0000\n",
            "Epoch: 007/010 | Batch 0400/2466 | Cost: 0.0001\n",
            "Epoch: 007/010 | Batch 0450/2466 | Cost: 0.0000\n",
            "Epoch: 007/010 | Batch 0500/2466 | Cost: 0.0001\n",
            "Epoch: 007/010 | Batch 0550/2466 | Cost: 0.0000\n",
            "Epoch: 007/010 | Batch 0600/2466 | Cost: 0.0018\n",
            "Epoch: 007/010 | Batch 0650/2466 | Cost: 0.0000\n",
            "Epoch: 007/010 | Batch 0700/2466 | Cost: 0.0001\n",
            "Epoch: 007/010 | Batch 0750/2466 | Cost: 0.0000\n",
            "Epoch: 007/010 | Batch 0800/2466 | Cost: 0.0018\n",
            "Epoch: 007/010 | Batch 0850/2466 | Cost: 0.0000\n",
            "Epoch: 007/010 | Batch 0900/2466 | Cost: 0.0013\n",
            "Epoch: 007/010 | Batch 0950/2466 | Cost: 0.0000\n",
            "Epoch: 007/010 | Batch 1000/2466 | Cost: 0.0008\n",
            "Epoch: 007/010 | Batch 1050/2466 | Cost: 0.0002\n",
            "Epoch: 007/010 | Batch 1100/2466 | Cost: 0.0000\n",
            "Epoch: 007/010 | Batch 1150/2466 | Cost: 0.0002\n",
            "Epoch: 007/010 | Batch 1200/2466 | Cost: 0.0000\n",
            "Epoch: 007/010 | Batch 1250/2466 | Cost: 0.0000\n",
            "Epoch: 007/010 | Batch 1300/2466 | Cost: 0.0000\n",
            "Epoch: 007/010 | Batch 1350/2466 | Cost: 0.0000\n",
            "Epoch: 007/010 | Batch 1400/2466 | Cost: 0.0000\n",
            "Epoch: 007/010 | Batch 1450/2466 | Cost: 0.0001\n",
            "Epoch: 007/010 | Batch 1500/2466 | Cost: 0.0000\n",
            "Epoch: 007/010 | Batch 1550/2466 | Cost: 0.0000\n",
            "Epoch: 007/010 | Batch 1600/2466 | Cost: 0.0000\n",
            "Epoch: 007/010 | Batch 1650/2466 | Cost: 0.0000\n",
            "Epoch: 007/010 | Batch 1700/2466 | Cost: 0.0012\n",
            "Epoch: 007/010 | Batch 1750/2466 | Cost: 0.0000\n",
            "Epoch: 007/010 | Batch 1800/2466 | Cost: 0.0000\n",
            "Epoch: 007/010 | Batch 1850/2466 | Cost: 0.0003\n",
            "Epoch: 007/010 | Batch 1900/2466 | Cost: 0.0011\n",
            "Epoch: 007/010 | Batch 1950/2466 | Cost: 0.0010\n",
            "Epoch: 007/010 | Batch 2000/2466 | Cost: 0.0019\n",
            "Epoch: 007/010 | Batch 2050/2466 | Cost: 0.0038\n",
            "Epoch: 007/010 | Batch 2100/2466 | Cost: 0.0000\n",
            "Epoch: 007/010 | Batch 2150/2466 | Cost: 0.0035\n",
            "Epoch: 007/010 | Batch 2200/2466 | Cost: 0.0000\n",
            "Epoch: 007/010 | Batch 2250/2466 | Cost: 0.0001\n",
            "Epoch: 007/010 | Batch 2300/2466 | Cost: 0.0000\n",
            "Epoch: 007/010 | Batch 2350/2466 | Cost: 0.0005\n",
            "Epoch: 007/010 | Batch 2400/2466 | Cost: 0.0000\n",
            "Epoch: 007/010 | Batch 2450/2466 | Cost: 0.0000\n",
            "Epoch: 007/010 | Train: 0.995%   | Val: 0.997%\n",
            "Time elapsed: 22.40 min\n",
            "Epoch: 008/010 | Batch 0000/2466 | Cost: 0.0010\n",
            "Epoch: 008/010 | Batch 0050/2466 | Cost: 0.0006\n",
            "Epoch: 008/010 | Batch 0100/2466 | Cost: 0.0000\n",
            "Epoch: 008/010 | Batch 0150/2466 | Cost: 0.0000\n",
            "Epoch: 008/010 | Batch 0200/2466 | Cost: 0.0000\n",
            "Epoch: 008/010 | Batch 0250/2466 | Cost: 0.0000\n",
            "Epoch: 008/010 | Batch 0300/2466 | Cost: 0.0000\n",
            "Epoch: 008/010 | Batch 0350/2466 | Cost: 0.0002\n",
            "Epoch: 008/010 | Batch 0400/2466 | Cost: 0.0001\n",
            "Epoch: 008/010 | Batch 0450/2466 | Cost: 0.0000\n",
            "Epoch: 008/010 | Batch 0500/2466 | Cost: 0.0000\n",
            "Epoch: 008/010 | Batch 0550/2466 | Cost: 0.0013\n",
            "Epoch: 008/010 | Batch 0600/2466 | Cost: 0.0014\n",
            "Epoch: 008/010 | Batch 0650/2466 | Cost: 0.0000\n",
            "Epoch: 008/010 | Batch 0700/2466 | Cost: 0.0014\n",
            "Epoch: 008/010 | Batch 0750/2466 | Cost: 0.0000\n",
            "Epoch: 008/010 | Batch 0800/2466 | Cost: 0.0133\n",
            "Epoch: 008/010 | Batch 0850/2466 | Cost: 0.0000\n",
            "Epoch: 008/010 | Batch 0900/2466 | Cost: 0.0001\n",
            "Epoch: 008/010 | Batch 0950/2466 | Cost: 0.0012\n",
            "Epoch: 008/010 | Batch 1000/2466 | Cost: 0.0000\n",
            "Epoch: 008/010 | Batch 1050/2466 | Cost: 0.0012\n",
            "Epoch: 008/010 | Batch 1100/2466 | Cost: 0.0000\n",
            "Epoch: 008/010 | Batch 1150/2466 | Cost: 0.0001\n",
            "Epoch: 008/010 | Batch 1200/2466 | Cost: 0.0009\n",
            "Epoch: 008/010 | Batch 1250/2466 | Cost: 0.0008\n",
            "Epoch: 008/010 | Batch 1300/2466 | Cost: 0.0000\n",
            "Epoch: 008/010 | Batch 1350/2466 | Cost: 0.0000\n",
            "Epoch: 008/010 | Batch 1400/2466 | Cost: 0.0002\n",
            "Epoch: 008/010 | Batch 1450/2466 | Cost: 0.0000\n",
            "Epoch: 008/010 | Batch 1500/2466 | Cost: 0.0004\n",
            "Epoch: 008/010 | Batch 1550/2466 | Cost: 0.0000\n",
            "Epoch: 008/010 | Batch 1600/2466 | Cost: 0.0010\n",
            "Epoch: 008/010 | Batch 1650/2466 | Cost: 0.0000\n",
            "Epoch: 008/010 | Batch 1700/2466 | Cost: 0.0019\n",
            "Epoch: 008/010 | Batch 1750/2466 | Cost: 0.0000\n",
            "Epoch: 008/010 | Batch 1800/2466 | Cost: 0.0000\n",
            "Epoch: 008/010 | Batch 1850/2466 | Cost: 0.0000\n",
            "Epoch: 008/010 | Batch 1900/2466 | Cost: 0.0283\n",
            "Epoch: 008/010 | Batch 1950/2466 | Cost: 0.0000\n",
            "Epoch: 008/010 | Batch 2000/2466 | Cost: 0.0006\n",
            "Epoch: 008/010 | Batch 2050/2466 | Cost: 0.0000\n",
            "Epoch: 008/010 | Batch 2100/2466 | Cost: 0.0000\n",
            "Epoch: 008/010 | Batch 2150/2466 | Cost: 0.0641\n",
            "Epoch: 008/010 | Batch 2200/2466 | Cost: 0.0000\n",
            "Epoch: 008/010 | Batch 2250/2466 | Cost: 0.0001\n",
            "Epoch: 008/010 | Batch 2300/2466 | Cost: 0.0000\n",
            "Epoch: 008/010 | Batch 2350/2466 | Cost: 0.0000\n",
            "Epoch: 008/010 | Batch 2400/2466 | Cost: 0.0000\n",
            "Epoch: 008/010 | Batch 2450/2466 | Cost: 0.0001\n",
            "Epoch: 008/010 | Train: 0.996%   | Val: 0.996%\n",
            "Time elapsed: 25.73 min\n",
            "Epoch: 009/010 | Batch 0000/2466 | Cost: 0.0000\n",
            "Epoch: 009/010 | Batch 0050/2466 | Cost: 0.0001\n",
            "Epoch: 009/010 | Batch 0100/2466 | Cost: 0.0000\n",
            "Epoch: 009/010 | Batch 0150/2466 | Cost: 0.0000\n",
            "Epoch: 009/010 | Batch 0200/2466 | Cost: 0.0001\n",
            "Epoch: 009/010 | Batch 0250/2466 | Cost: 0.0000\n",
            "Epoch: 009/010 | Batch 0300/2466 | Cost: 0.0001\n",
            "Epoch: 009/010 | Batch 0350/2466 | Cost: 0.0000\n",
            "Epoch: 009/010 | Batch 0400/2466 | Cost: 0.0000\n",
            "Epoch: 009/010 | Batch 0450/2466 | Cost: 0.0000\n",
            "Epoch: 009/010 | Batch 0500/2466 | Cost: 0.0016\n",
            "Epoch: 009/010 | Batch 0550/2466 | Cost: 0.0000\n",
            "Epoch: 009/010 | Batch 0600/2466 | Cost: 0.0000\n",
            "Epoch: 009/010 | Batch 0650/2466 | Cost: 0.0000\n",
            "Epoch: 009/010 | Batch 0700/2466 | Cost: 0.0000\n",
            "Epoch: 009/010 | Batch 0750/2466 | Cost: 0.0068\n",
            "Epoch: 009/010 | Batch 0800/2466 | Cost: 0.0001\n",
            "Epoch: 009/010 | Batch 0850/2466 | Cost: 0.0000\n",
            "Epoch: 009/010 | Batch 0900/2466 | Cost: 0.0000\n",
            "Epoch: 009/010 | Batch 0950/2466 | Cost: 0.0000\n",
            "Epoch: 009/010 | Batch 1000/2466 | Cost: 0.0000\n",
            "Epoch: 009/010 | Batch 1050/2466 | Cost: 0.0000\n",
            "Epoch: 009/010 | Batch 1100/2466 | Cost: 0.0117\n",
            "Epoch: 009/010 | Batch 1150/2466 | Cost: 0.0000\n",
            "Epoch: 009/010 | Batch 1200/2466 | Cost: 0.0001\n",
            "Epoch: 009/010 | Batch 1250/2466 | Cost: 0.0000\n",
            "Epoch: 009/010 | Batch 1300/2466 | Cost: 0.0013\n",
            "Epoch: 009/010 | Batch 1350/2466 | Cost: 0.0000\n",
            "Epoch: 009/010 | Batch 1400/2466 | Cost: 0.0000\n",
            "Epoch: 009/010 | Batch 1450/2466 | Cost: 0.0005\n",
            "Epoch: 009/010 | Batch 1500/2466 | Cost: 0.0000\n",
            "Epoch: 009/010 | Batch 1550/2466 | Cost: 0.0000\n",
            "Epoch: 009/010 | Batch 1600/2466 | Cost: 0.0000\n",
            "Epoch: 009/010 | Batch 1650/2466 | Cost: 0.0000\n",
            "Epoch: 009/010 | Batch 1700/2466 | Cost: 0.0118\n",
            "Epoch: 009/010 | Batch 1750/2466 | Cost: 0.0000\n",
            "Epoch: 009/010 | Batch 1800/2466 | Cost: 0.0000\n",
            "Epoch: 009/010 | Batch 1850/2466 | Cost: 0.0000\n",
            "Epoch: 009/010 | Batch 1900/2466 | Cost: 0.0008\n",
            "Epoch: 009/010 | Batch 1950/2466 | Cost: 0.0000\n",
            "Epoch: 009/010 | Batch 2000/2466 | Cost: 0.0147\n",
            "Epoch: 009/010 | Batch 2050/2466 | Cost: 0.0001\n",
            "Epoch: 009/010 | Batch 2100/2466 | Cost: 0.0001\n",
            "Epoch: 009/010 | Batch 2150/2466 | Cost: 0.0011\n",
            "Epoch: 009/010 | Batch 2200/2466 | Cost: 0.0000\n",
            "Epoch: 009/010 | Batch 2250/2466 | Cost: 0.0009\n",
            "Epoch: 009/010 | Batch 2300/2466 | Cost: 0.0000\n",
            "Epoch: 009/010 | Batch 2350/2466 | Cost: 0.0000\n",
            "Epoch: 009/010 | Batch 2400/2466 | Cost: 0.0001\n",
            "Epoch: 009/010 | Batch 2450/2466 | Cost: 0.0000\n",
            "Epoch: 009/010 | Train: 0.996%   | Val: 0.998%\n",
            "Time elapsed: 28.93 min\n",
            "Epoch: 010/010 | Batch 0000/2466 | Cost: 0.0000\n",
            "Epoch: 010/010 | Batch 0050/2466 | Cost: 0.0472\n",
            "Epoch: 010/010 | Batch 0100/2466 | Cost: 0.0000\n",
            "Epoch: 010/010 | Batch 0150/2466 | Cost: 0.0000\n",
            "Epoch: 010/010 | Batch 0200/2466 | Cost: 0.0000\n",
            "Epoch: 010/010 | Batch 0250/2466 | Cost: 0.0000\n",
            "Epoch: 010/010 | Batch 0300/2466 | Cost: 0.0001\n",
            "Epoch: 010/010 | Batch 0350/2466 | Cost: 0.0000\n",
            "Epoch: 010/010 | Batch 0400/2466 | Cost: 0.0019\n",
            "Epoch: 010/010 | Batch 0450/2466 | Cost: 0.0000\n",
            "Epoch: 010/010 | Batch 0500/2466 | Cost: 0.0004\n",
            "Epoch: 010/010 | Batch 0550/2466 | Cost: 0.0032\n",
            "Epoch: 010/010 | Batch 0600/2466 | Cost: 0.0002\n",
            "Epoch: 010/010 | Batch 0650/2466 | Cost: 0.0000\n",
            "Epoch: 010/010 | Batch 0700/2466 | Cost: 0.0000\n",
            "Epoch: 010/010 | Batch 0750/2466 | Cost: 0.0001\n",
            "Epoch: 010/010 | Batch 0800/2466 | Cost: 0.0001\n",
            "Epoch: 010/010 | Batch 0850/2466 | Cost: 0.0004\n",
            "Epoch: 010/010 | Batch 0900/2466 | Cost: 0.0000\n",
            "Epoch: 010/010 | Batch 0950/2466 | Cost: 0.0000\n",
            "Epoch: 010/010 | Batch 1000/2466 | Cost: 0.0000\n",
            "Epoch: 010/010 | Batch 1050/2466 | Cost: 0.0000\n",
            "Epoch: 010/010 | Batch 1100/2466 | Cost: 0.0003\n",
            "Epoch: 010/010 | Batch 1150/2466 | Cost: 0.0000\n",
            "Epoch: 010/010 | Batch 1200/2466 | Cost: 0.0001\n",
            "Epoch: 010/010 | Batch 1250/2466 | Cost: 0.0007\n",
            "Epoch: 010/010 | Batch 1300/2466 | Cost: 0.0000\n",
            "Epoch: 010/010 | Batch 1350/2466 | Cost: 0.0000\n",
            "Epoch: 010/010 | Batch 1400/2466 | Cost: 0.0000\n",
            "Epoch: 010/010 | Batch 1450/2466 | Cost: 0.0000\n",
            "Epoch: 010/010 | Batch 1500/2466 | Cost: 0.0000\n",
            "Epoch: 010/010 | Batch 1550/2466 | Cost: 0.0000\n",
            "Epoch: 010/010 | Batch 1600/2466 | Cost: 0.0000\n",
            "Epoch: 010/010 | Batch 1650/2466 | Cost: 0.0187\n",
            "Epoch: 010/010 | Batch 1700/2466 | Cost: 0.0000\n",
            "Epoch: 010/010 | Batch 1750/2466 | Cost: 0.0006\n",
            "Epoch: 010/010 | Batch 1800/2466 | Cost: 0.0000\n",
            "Epoch: 010/010 | Batch 1850/2466 | Cost: 0.0000\n",
            "Epoch: 010/010 | Batch 1900/2466 | Cost: 0.0008\n",
            "Epoch: 010/010 | Batch 1950/2466 | Cost: 0.0001\n",
            "Epoch: 010/010 | Batch 2000/2466 | Cost: 0.0023\n",
            "Epoch: 010/010 | Batch 2050/2466 | Cost: 0.0000\n",
            "Epoch: 010/010 | Batch 2100/2466 | Cost: 0.0000\n",
            "Epoch: 010/010 | Batch 2150/2466 | Cost: 0.0004\n",
            "Epoch: 010/010 | Batch 2200/2466 | Cost: 0.0000\n",
            "Epoch: 010/010 | Batch 2250/2466 | Cost: 0.0001\n",
            "Epoch: 010/010 | Batch 2300/2466 | Cost: 0.0000\n",
            "Epoch: 010/010 | Batch 2350/2466 | Cost: 0.0000\n",
            "Epoch: 010/010 | Batch 2400/2466 | Cost: 0.0069\n",
            "Epoch: 010/010 | Batch 2450/2466 | Cost: 0.0221\n",
            "Epoch: 010/010 | Train: 0.996%   | Val: 0.998%\n",
            "Time elapsed: 32.17 min\n",
            "Total Training Time: 32.17 min\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 541
        },
        "id": "EGRqtOSy-yU5",
        "outputId": "683d4baf-d6d9-4e1c-dc3e-b55153e50567"
      },
      "source": [
        "# plot cost functions\n",
        "minibatch_cost_cpu = [i.cpu().detach().numpy() for i in minibatch_cost]\n",
        "# epoch_cost_cpu = [i.cpu().detach().numpy() for i in epoch_performance]\n",
        "\n",
        "\n",
        "plt.plot(range(len(minibatch_cost_cpu)), minibatch_cost_cpu)\n",
        "plt.ylabel('Cost Function Label') # update this label after cost function selected\n",
        "plt.xlabel('Minibatch')\n",
        "plt.show()\n",
        "\n",
        "plt.plot(range(len(epoch_train_performance)), epoch_train_performance, label=\"train f1 scores\")\n",
        "plt.plot(range(len(epoch_val_performance)), epoch_val_performance, label=\"val f1 scores\")\n",
        "plt.ylabel('F1 Score') # update this label after cost function selected\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left', borderaxespad=0.)\n",
        "plt.show()"
      ],
      "execution_count": 120,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "obJPMecX-yU6"
      },
      "source": [
        "## Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": [
          "outputPrepend"
        ],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3nixqH2I-yU6",
        "outputId": "6c80abd1-f475-4288-c2b0-683a184fc494"
      },
      "source": [
        "with torch.set_grad_enabled(False): # save memory during inference\n",
        "    # THIS NEEDS TO BE UPDATED BASED ON THE FUNCTION NAME WE'RE USING\n",
        "    print('Test f1_score: %.2f%%' % (compute_f1(model, test_loader, device=DEVICE)))"
      ],
      "execution_count": 84,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test f1_score: 1.00%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K2MybMVy-yU6",
        "outputId": "d999ac73-f75a-4f50-ab21-cbcd7bd8e36f"
      },
      "source": [
        "with torch.set_grad_enabled(False): # save memory during inference\n",
        "    # THIS NEEDS TO BE UPDATED BASED ON THE FUNCTION NAME WE'RE USING\n",
        "    print('Train f1_score: %.2f%%' % (compute_f1(model, train_loader, device=DEVICE)))"
      ],
      "execution_count": 85,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train f1_score: 0.99%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dyPBdbpM-yU6"
      },
      "source": [
        "# WE NEED TO BE EXTENDING THE PERFORMANCE MEASURES TO INCLUDE THOSE USED IN THE PAPERS\n",
        "# F SCORE, PRECISION ETC ETC, MOST OF THE PAPERS SEEM TO USE SIMILAR METRICS, SHOULD BE RELATIVELY SIMPLE\n",
        "# AND COULD BE POTENTIALLY BUILD DIRECTLY INTO THE COST FUNCTION (THE FUNCTION COULD RETURN MULTIPLE PARAMS AND WE WOULD OBVIOUSLY JUST USE THE ONE WE NEEDED FOR MODEL TRAINING BUT THEN WE WOULD HAVE ACCESS TO ALL OF THEM INSTEAD OF WRITING ANOTHER FUNCTION)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TabX97SN-yU7"
      },
      "source": [
        "# ALSO RECOMMEND SOME SORT OF VIS TOOL ON BEING ABLE TO LOOK AT A COUPLE OF THE TIME IMAGES THAT ARE LABELLED ANOMALOUS CORRECTLY AND ONES THAT ARE LABELLED ANOMLAOUS INCORRECTLY OR NORMAL INCORRECTLY"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j80T4jkJ-yU7",
        "outputId": "7f7f4ae3-909d-4543-d935-c0a5ae683ac7"
      },
      "source": [
        "# Evaluate F1 score on val set\n",
        "y_hats = []\n",
        "y_acts = []\n",
        "for i, (inputs, targets) in enumerate(val_loader):\n",
        "    yhat = model(inputs)[-1].cpu().detach().numpy().round()\n",
        "    yhat = np.argmax(yhat, axis=1)\n",
        "    y_hats.append(yhat)\n",
        "    y_acts.append(list(targets.cpu().detach().numpy()))\n",
        "\n",
        "y_hats = [item for sublist in y_hats for item in sublist]\n",
        "y_acts = [item for sublist in y_acts for item in sublist]\n",
        "\n",
        "f1 = f1_score(y_acts, y_hats)\n",
        "print(f1)\n"
      ],
      "execution_count": 152,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.9977977977977978\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "03nz_tXO-yU7",
        "outputId": "adf8f4e8-0185-4dc2-f4be-6d33c0d1a1ad"
      },
      "source": [
        "# Evaluate F1 score on test set\n",
        "y_hats = []\n",
        "y_acts = []\n",
        "counter = 0\n",
        "for i, (inputs, targets) in enumerate(test_loader):\n",
        "    yhat = model(inputs)[-1].cpu().detach().numpy().round()\n",
        "    yhat = np.argmax(yhat, axis=1)\n",
        "    y_hats.append(yhat)\n",
        "    y_acts.append(list(targets.cpu().detach().numpy()))\n",
        "    counter += 1\n",
        "\n",
        "y_hats = [item for sublist in y_hats for item in sublist]\n",
        "y_acts = [item for sublist in y_acts for item in sublist]\n",
        "\n",
        "f1 = f1_score(y_acts, y_hats)\n",
        "print(f1)"
      ],
      "execution_count": 153,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.9989914271306102\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nUuzCDQD-yU8"
      },
      "source": [
        "from sklearn.metrics import precision_score\n",
        "from sklearn.metrics import recall_score\n",
        "from sklearn.metrics import confusion_matrix"
      ],
      "execution_count": 154,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jcDc0JaI-yU8",
        "outputId": "33ab3ee7-ed50-42cd-9524-b641155f8c29"
      },
      "source": [
        "precision = precision_score(y_acts, y_hats)\n",
        "print(precision)"
      ],
      "execution_count": 155,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.9989914271306102\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZSPlGBPe-yU8",
        "outputId": "9505c043-f297-4ca2-c073-3ad8aac0e499"
      },
      "source": [
        "recall = recall_score(y_acts, y_hats)\n",
        "print(recall)"
      ],
      "execution_count": 156,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.9989914271306102\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n6txV-0N-yU9",
        "outputId": "6276cf95-3c97-4860-daed-0d70bbc7c592"
      },
      "source": [
        "f1_score(y_acts, y_hats)"
      ],
      "execution_count": 157,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9989914271306102"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 157
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pix1J1yL-yU9",
        "outputId": "8d4e59d9-7ec4-448a-caf2-40d5bc824b29"
      },
      "source": [
        "confusion_matrix(y_acts, y_hats)"
      ],
      "execution_count": 158,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[118552,      2],\n",
              "       [     2,   1981]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 158
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        },
        "id": "DtHll0VHq5ZF",
        "outputId": "0d1e31c7-5450-4613-faf2-f3e5b9d563b4"
      },
      "source": [
        "test_ys = pd.DataFrame(list(zip(y_acts, y_hats)), columns=[\"y_true\", \"y_pred\"])\n",
        "\n",
        "test_ys"
      ],
      "execution_count": 159,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>y_true</th>\n",
              "      <th>y_pred</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>120532</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>120533</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>120534</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>120535</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>120536</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>120537 rows × 2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "        y_true  y_pred\n",
              "0            0       0\n",
              "1            0       0\n",
              "2            0       0\n",
              "3            0       0\n",
              "4            0       0\n",
              "...        ...     ...\n",
              "120532       0       0\n",
              "120533       0       0\n",
              "120534       0       0\n",
              "120535       0       0\n",
              "120536       1       1\n",
              "\n",
              "[120537 rows x 2 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 159
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h3_a-OxwsJwp",
        "outputId": "cc037828-1f18-48ff-8c76-cd017201245b"
      },
      "source": [
        "print(\"TEST SET:\\n\")\n",
        "print(\"anomalous:\\n\")\n",
        "test_anomalous = test_ys[test_ys[\"y_true\"]==1]\n",
        "print(\"number of anomalies in the test set:\", len(test_anomalous))\n",
        "correct_anomalous = test_anomalous[test_anomalous[\"y_true\"] == test_anomalous[\"y_pred\"]]\n",
        "print(\"number of anomalies correctly identified\", len(correct_anomalous))\n",
        "incorrect_anomalous = test_anomalous[test_anomalous[\"y_true\"] != test_anomalous[\"y_pred\"]]\n",
        "print(\"number of anomalies incorrectly identified\", len(incorrect_anomalous))\n",
        "\n",
        "print(\"\\nnormal:\\n\")\n",
        "test_normals = test_ys[test_ys[\"y_true\"]==0]\n",
        "print(\"number of normals in the test set:\", len(test_normals))\n",
        "correct_normal = test_normals[test_normals[\"y_true\"] == test_normals[\"y_pred\"]]\n",
        "print(\"number of normals correctly identified\", len(correct_normal))\n",
        "incorrect_normal = test_normals[test_normals[\"y_true\"] != test_normals[\"y_pred\"]]\n",
        "print(\"number of normals correctly identified\", len(incorrect_normal))"
      ],
      "execution_count": 174,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TEST SET:\n",
            "\n",
            "anomalous:\n",
            "\n",
            "number of anomalies in the test set: 1983\n",
            "number of anomalies correctly identified 1981\n",
            "number of anomalies incorrectly identified 2\n",
            "\n",
            "normal:\n",
            "\n",
            "number of normals in the test set: 118554\n",
            "number of normals correctly identified 118552\n",
            "number of normals correctly identified 2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IW4Z3-S5pPsR",
        "outputId": "040a6a3e-39af-48ee-cf66-4e00414e05f5"
      },
      "source": [
        "# Evaluate F1 score on train set\n",
        "y_hats = []\n",
        "y_acts = []\n",
        "counter = 0\n",
        "for i, (inputs, targets) in enumerate(train_loader):\n",
        "    yhat = model(inputs)[-1].cpu().detach().numpy().round()\n",
        "    yhat = np.argmax(yhat, axis=1)\n",
        "    y_hats.append(yhat)\n",
        "    y_acts.append(list(targets.cpu().detach().numpy()))\n",
        "    counter += 1\n",
        "\n",
        "y_hats = [item for sublist in y_hats for item in sublist]\n",
        "y_acts = [item for sublist in y_acts for item in sublist]\n",
        "\n",
        "f1 = f1_score(y_acts, y_hats)\n",
        "print(f1)"
      ],
      "execution_count": 179,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.9961429151441332\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        },
        "id": "sH5Ud-sEu0x4",
        "outputId": "c72e0d36-11c3-4473-bf12-563681f5c953"
      },
      "source": [
        "train_ys = pd.DataFrame(list(zip(y_acts, y_hats)), columns=[\"y_true\", \"y_pred\"])\n",
        "\n",
        "train_ys"
      ],
      "execution_count": 180,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>y_true</th>\n",
              "      <th>y_pred</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>315597</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>315598</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>315599</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>315600</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>315601</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>315602 rows × 2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "        y_true  y_pred\n",
              "0            0       0\n",
              "1            0       0\n",
              "2            0       0\n",
              "3            0       0\n",
              "4            0       0\n",
              "...        ...     ...\n",
              "315597       0       0\n",
              "315598       0       0\n",
              "315599       0       0\n",
              "315600       0       0\n",
              "315601       0       0\n",
              "\n",
              "[315602 rows x 2 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 180
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VXYE_-X1pqxL",
        "outputId": "36709598-cce0-4a59-ff33-0ece03d0721b"
      },
      "source": [
        "precision = precision_score(y_acts, y_hats)\n",
        "print(precision)"
      ],
      "execution_count": 181,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.9934203866788136\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LMT3QVvspslX",
        "outputId": "b55f54bd-05ce-471d-ef01-7429be83cef6"
      },
      "source": [
        "recall = recall_score(y_acts, y_hats)\n",
        "print(recall)"
      ],
      "execution_count": 182,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.998880407124682\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wKCozDOept2v",
        "outputId": "5417d54b-7c8c-418a-bfa1-f0ec17479a8e"
      },
      "source": [
        "confusion_matrix(y_acts, y_hats)"
      ],
      "execution_count": 183,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[305712,     65],\n",
              "       [    11,   9814]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 183
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aqZLZHRRpv-3",
        "outputId": "d98361ea-6506-4573-e5b5-c2115eac0065"
      },
      "source": [
        "print(\"TRAIN SET:\\n\")\n",
        "print(\"anomalous:\\n\")\n",
        "train_anomalous = train_ys[train_ys[\"y_true\"]==1]\n",
        "print(\"number of anomalies in the train set:\", len(train_anomalous))\n",
        "correct_anomalous = train_anomalous[train_anomalous[\"y_true\"] == train_anomalous[\"y_pred\"]]\n",
        "print(\"number of anomalies correctly identified\", len(correct_anomalous))\n",
        "incorrect_anomalous = train_anomalous[train_anomalous[\"y_true\"] != train_anomalous[\"y_pred\"]]\n",
        "print(\"number of anomalies incorrectly identified\", len(incorrect_anomalous))\n",
        "\n",
        "print(\"\\nnormal:\\n\")\n",
        "train_normals = train_ys[train_ys[\"y_true\"]==0]\n",
        "print(\"number of normals in the train set:\", len(train_normals))\n",
        "correct_normal = train_normals[train_normals[\"y_true\"] == train_normals[\"y_pred\"]]\n",
        "print(\"number of normals correctly identified\", len(correct_normal))\n",
        "incorrect_normal = train_normals[train_normals[\"y_true\"] != train_normals[\"y_pred\"]]\n",
        "print(\"number of normals correctly identified\", len(incorrect_normal))"
      ],
      "execution_count": 184,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TRAIN SET:\n",
            "\n",
            "anomalous:\n",
            "\n",
            "number of anomalies in the train set: 9825\n",
            "number of anomalies correctly identified 9814\n",
            "number of anomalies incorrectly identified 11\n",
            "\n",
            "normal:\n",
            "\n",
            "number of normals in the train set: 305777\n",
            "number of normals correctly identified 305712\n",
            "number of normals correctly identified 65\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SN9sqHAiu3LG"
      },
      "source": [
        ""
      ],
      "execution_count": 184,
      "outputs": []
    }
  ]
}