# Process Folder

Contains code to extract features from the semi-structured log files produced by the files contained within the `parse` directory. The data generated in this folder is used directly by the CNN model.

## Feature Extraction Overview

The code in this directory borrows heavily from the feature extraction functionality of the [Loglizer](https://github.com/logpai/loglizer) project. 

+ Shilin He, Jieming Zhu, Pinjia He, Michael R. Lyu. [Experience Report: System Log Analysis for Anomaly Detection](https://jiemingzhu.github.io/pub/slhe_issre2016.pdf), *IEEE International Symposium on Software Reliability Engineering (ISSRE)*, 2016.


Loglizer takes semi-structured log files and turns them into a dataframe, with one column as the block ids and another column the block's events stored as a list. Then turns the event lists into a bag of words, with options for normalization and TF-IDF, to be fed into a predictive model.

This repository uses the framework of Loglizer's feature extraction but adds the following functionality:

- Slices each block event list into multiple rows with each row representing a 5% increment and creates a 2D array for each block event list (rows=5% increments, columns=Events). Each cell in the 2D array represents a count for an event with the 5% increment (essentially Bag of Words of events)
- TF-IDF is then performed on the 2D arrays using the.
- The 2D arrays are then modified such that each row represents a rolling 5% increment. Instead of rows representing 0-5%, 5-10%, 10-15%, etc, the rows now represent 0-10%, 5-15%, 10-20%, etc. 
- The output is a 2D array for each block.

## Data and Scripts

The following provides a description of the files located in this folder:

- `data_processor.py` contains the functions used to generate the block event features as described above.

- `project_processor.py` uses the functions in `data_processor.py` to extract features from the data generated by `/parse/project_parser.py` and located in the `/parse/project_parsed` folder. Specifically, the files called by `project_processor.py` are:
    - `anomaly_label.csv` (anomalous/normal labels for HDFS blocks)
    - `HDFS_train.log_structured.csv` (training data)
    - `HDFS_test.log_structured.csv` (testing data)
- The processed data is then stored in the `project_processed_data` folder as:
    - `x_train_v2.npy`
    - `x_test_v2.npy`
    - `y_train_v2.csv`
    - `y_test_v2.csv`
- `process_demo.ipynb` provides a demo of using the processing script and is likely the best way to understand the feature extraction code at a high-level.

- `test_data_processor.py` provides a small number of tests for `data_processor.py` and `check_processed_data.py` is used to check that the output after running `data_processor.py` is in the correct format. `check_processed_data.py` may be removed in the future.

- `example_data` and `testing.ipynb` provide example data files used when building/testing the `data_processor.py` script (these may be removed in the future).

