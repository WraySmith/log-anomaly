{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering\n",
    "\n",
    "The initial concept and some of the code borrows heavily from [Loglizer](https://github.com/logpai/loglizer)  \n",
    "Input data for the `data_processor.py` file is created by `parse/project_parser.py` and is stored in `parse/project_parsed`\n",
    "\n",
    "This demo will walk you through the process of converting the semi-structured log data created by the `parse` files into `image images` to be fed into a CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import OrderedDict\n",
    "import regex as re\n",
    "from data_processor import FeatureExtractor\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The train set is loaded to look at the format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data = pd.read_csv(\"../parse/project_parsed/HDFS_train.log_structured.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now load the y data, and subset the x for easy demonstration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = pd.read_csv(\"../parse/project_parsed/anomaly_label.csv\")\n",
    "x_train = input_data.loc[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>LineId</th>\n",
       "      <th>Date</th>\n",
       "      <th>Time</th>\n",
       "      <th>Pid</th>\n",
       "      <th>Level</th>\n",
       "      <th>Component</th>\n",
       "      <th>Content</th>\n",
       "      <th>EventId</th>\n",
       "      <th>EventTemplate</th>\n",
       "      <th>ParameterList</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>81109</td>\n",
       "      <td>203518</td>\n",
       "      <td>143</td>\n",
       "      <td>INFO</td>\n",
       "      <td>dfs.DataNode$DataXceiver</td>\n",
       "      <td>Receiving block blk_-1608999687919862906 src: ...</td>\n",
       "      <td>09a53393</td>\n",
       "      <td>Receiving block &lt;*&gt; src: &lt;*&gt; dest: &lt;*&gt;</td>\n",
       "      <td>['blk_-1608999687919862906', '/10.250.19.102:5...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>81109</td>\n",
       "      <td>203518</td>\n",
       "      <td>35</td>\n",
       "      <td>INFO</td>\n",
       "      <td>dfs.FSNamesystem</td>\n",
       "      <td>BLOCK* NameSystem.allocateBlock: /mnt/hadoop/m...</td>\n",
       "      <td>3d91fa85</td>\n",
       "      <td>BLOCK* NameSystem.allocateBlock: &lt;*&gt; &lt;*&gt;</td>\n",
       "      <td>['/mnt/hadoop/mapred/system/job_200811092030_0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>81109</td>\n",
       "      <td>203519</td>\n",
       "      <td>143</td>\n",
       "      <td>INFO</td>\n",
       "      <td>dfs.DataNode$DataXceiver</td>\n",
       "      <td>Receiving block blk_-1608999687919862906 src: ...</td>\n",
       "      <td>09a53393</td>\n",
       "      <td>Receiving block &lt;*&gt; src: &lt;*&gt; dest: &lt;*&gt;</td>\n",
       "      <td>['blk_-1608999687919862906', '/10.250.10.6:405...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>81109</td>\n",
       "      <td>203519</td>\n",
       "      <td>145</td>\n",
       "      <td>INFO</td>\n",
       "      <td>dfs.DataNode$DataXceiver</td>\n",
       "      <td>Receiving block blk_-1608999687919862906 src: ...</td>\n",
       "      <td>09a53393</td>\n",
       "      <td>Receiving block &lt;*&gt; src: &lt;*&gt; dest: &lt;*&gt;</td>\n",
       "      <td>['blk_-1608999687919862906', '/10.250.14.224:4...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>81109</td>\n",
       "      <td>203519</td>\n",
       "      <td>145</td>\n",
       "      <td>INFO</td>\n",
       "      <td>dfs.DataNode$PacketResponder</td>\n",
       "      <td>PacketResponder 1 for block blk_-1608999687919...</td>\n",
       "      <td>d38aa58d</td>\n",
       "      <td>PacketResponder &lt;*&gt; for block &lt;*&gt; &lt;*&gt;</td>\n",
       "      <td>['1', 'blk_-1608999687919862906 terminating']</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   LineId   Date    Time  Pid Level                     Component  \\\n",
       "0       1  81109  203518  143  INFO      dfs.DataNode$DataXceiver   \n",
       "1       2  81109  203518   35  INFO              dfs.FSNamesystem   \n",
       "2       3  81109  203519  143  INFO      dfs.DataNode$DataXceiver   \n",
       "3       4  81109  203519  145  INFO      dfs.DataNode$DataXceiver   \n",
       "4       5  81109  203519  145  INFO  dfs.DataNode$PacketResponder   \n",
       "\n",
       "                                             Content   EventId  \\\n",
       "0  Receiving block blk_-1608999687919862906 src: ...  09a53393   \n",
       "1  BLOCK* NameSystem.allocateBlock: /mnt/hadoop/m...  3d91fa85   \n",
       "2  Receiving block blk_-1608999687919862906 src: ...  09a53393   \n",
       "3  Receiving block blk_-1608999687919862906 src: ...  09a53393   \n",
       "4  PacketResponder 1 for block blk_-1608999687919...  d38aa58d   \n",
       "\n",
       "                              EventTemplate  \\\n",
       "0    Receiving block <*> src: <*> dest: <*>   \n",
       "1  BLOCK* NameSystem.allocateBlock: <*> <*>   \n",
       "2    Receiving block <*> src: <*> dest: <*>   \n",
       "3    Receiving block <*> src: <*> dest: <*>   \n",
       "4     PacketResponder <*> for block <*> <*>   \n",
       "\n",
       "                                       ParameterList  \n",
       "0  ['blk_-1608999687919862906', '/10.250.19.102:5...  \n",
       "1  ['/mnt/hadoop/mapred/system/job_200811092030_0...  \n",
       "2  ['blk_-1608999687919862906', '/10.250.10.6:405...  \n",
       "3  ['blk_-1608999687919862906', '/10.250.14.224:4...  \n",
       "4      ['1', 'blk_-1608999687919862906 terminating']  "
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, each event is collected into a list for each block id with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_event_ids(data_frame, regex_pattern, column_names):\n",
    "    \"\"\"\n",
    "    turns input data_frame into a 2 columned dataframe\n",
    "    with columns: BlockId, EventSequence\n",
    "    where EventSequence is a list of the events that happened to the block\n",
    "    \"\"\"\n",
    "    data_dict = OrderedDict()\n",
    "    for _, row in data_frame.iterrows():\n",
    "        blk_id_list = re.findall(regex_pattern, row[\"Content\"])\n",
    "        blk_id_set = set(blk_id_list)\n",
    "        for blk_id in blk_id_set:\n",
    "            if blk_id not in data_dict:\n",
    "                data_dict[blk_id] = []\n",
    "            data_dict[blk_id].append(row[\"EventId\"])\n",
    "    data_df = pd.DataFrame(list(data_dict.items()), columns=column_names)\n",
    "    return data_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "re_pat = r\"(blk_-?\\d+)\"\n",
    "col_names = [\"BlockId\", \"EventSequence\"]\n",
    "events_df = collect_event_ids(x_train, re_pat, col_names) # taking a subset for demonstrative purposes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This produced a dataframe with a unique identifier (BlockId) and the list of events in EventSequence  \n",
    "\n",
    "And now join with the y data, so the y data can become split into train and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>BlockId</th>\n",
       "      <th>EventSequence</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>blk_-1608999687919862906</td>\n",
       "      <td>[09a53393, 3d91fa85, 09a53393, 09a53393, d38aa...</td>\n",
       "      <td>Normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>blk_7503483334202473044</td>\n",
       "      <td>[09a53393, 09a53393, 3d91fa85, 09a53393, d38aa...</td>\n",
       "      <td>Normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>blk_-3544583377289625738</td>\n",
       "      <td>[09a53393, 3d91fa85, 09a53393, 09a53393, d38aa...</td>\n",
       "      <td>Anomaly</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>blk_-9073992586687739851</td>\n",
       "      <td>[09a53393, 3d91fa85, 09a53393, 09a53393, d38aa...</td>\n",
       "      <td>Normal</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    BlockId  \\\n",
       "0  blk_-1608999687919862906   \n",
       "1   blk_7503483334202473044   \n",
       "2  blk_-3544583377289625738   \n",
       "3  blk_-9073992586687739851   \n",
       "\n",
       "                                       EventSequence    Label  \n",
       "0  [09a53393, 3d91fa85, 09a53393, 09a53393, d38aa...   Normal  \n",
       "1  [09a53393, 09a53393, 3d91fa85, 09a53393, d38aa...   Normal  \n",
       "2  [09a53393, 3d91fa85, 09a53393, 09a53393, d38aa...  Anomaly  \n",
       "3  [09a53393, 3d91fa85, 09a53393, 09a53393, d38aa...   Normal  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "events_df = events_df.merge(y, on=\"BlockId\")\n",
    "display(events_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The EventSequence column is then passed to the feature extractor `fit_transform_subblocks()` method  \n",
    "To demonstrate what is happening in the class the code will be dissected and shown here step by step \n",
    "\n",
    "Note: `data_processor.py` also contains `fit_transform()` and `transform()`, these functions are to be used when you don't want to create time images (more on this to come)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data shape:  (4, 19, 10)\n"
     ]
    }
   ],
   "source": [
    "# The way the code is called normally\n",
    "events_values = events_df[\"EventSequence\"].values\n",
    "fe = FeatureExtractor()\n",
    "subblocks_train = fe.fit_transform_subblocks(\n",
    "        events_values, term_weighting=\"tf-idf\", rolling=True\n",
    "                )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we will define the parameters of the method call"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "rolling = True\n",
    "term_weighting = \"tf-idf\"\n",
    "\n",
    "X_seq = events_values\n",
    "unique_events = set()\n",
    "for i in X_seq:\n",
    "    unique_events.update(i)\n",
    "events = unique_events"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will be turning each event sequence into a time image.  \n",
    "This is done by splitting the event sequence into 5% increments, where each row is the 5% increment in time and the columns are the event sequence label.  \n",
    "Additionally when the rolling parameter is `True` the time image rows are rolling window summed, with window size of 2, to give the rows of our time image better temporal context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert into bag of words\n",
    "all_blocks_count = []\n",
    "for block in X_seq:\n",
    "    # multiply block by 20 for 5% partitions\n",
    "    block_rep = np.repeat(block, 20)\n",
    "    # now split into 5% partitions\n",
    "    block_split = np.split(block_rep, 20)\n",
    "    block_counts = []\n",
    "    for sub_block in block_split:\n",
    "        # count each sub_block\n",
    "        subset_count = Counter(sub_block)\n",
    "        block_counts.append(subset_count)\n",
    "    # put into dataframe to add nas to missing events\n",
    "    # divide by 20 as original operation multiplied by 20\n",
    "    block_df = pd.DataFrame(block_counts, columns=events) / 20\n",
    "    block_df = block_df.reindex(sorted(block_df.columns), axis=1)\n",
    "    block_df = block_df.fillna(0)\n",
    "    if rolling:\n",
    "        block_df = block_df.rolling(window=2).sum()\n",
    "        block_df = block_df.dropna()\n",
    "\n",
    "    block_np = block_df.to_numpy()\n",
    "    all_blocks_count.append(block_np)\n",
    "X = np.stack(all_blocks_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the first time image from the `X` numpy array:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4, 19, 10)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[3.  , 1.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.5 , 0.  , 0.  ],\n",
       "       [1.75, 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 2.  , 0.  , 0.75],\n",
       "       [0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 2.5 , 0.  , 2.  ],\n",
       "       [0.  , 0.  , 0.  , 1.25, 0.  , 0.  , 0.  , 1.  , 0.  , 2.25],\n",
       "       [0.  , 0.  , 0.  , 3.  , 0.  , 0.  , 0.  , 0.  , 0.5 , 1.  ],\n",
       "       [1.  , 0.  , 0.  , 1.75, 0.  , 0.75, 0.  , 0.  , 1.  , 0.  ],\n",
       "       [2.  , 0.  , 0.  , 0.  , 0.  , 1.  , 0.  , 0.  , 1.5 , 0.  ],\n",
       "       [1.  , 0.  , 1.  , 0.25, 0.  , 0.25, 1.  , 0.  , 1.  , 0.  ],\n",
       "       [0.  , 0.  , 1.  , 2.  , 0.5 , 0.  , 1.  , 0.  , 0.  , 0.  ],\n",
       "       [0.  , 0.  , 1.  , 1.75, 1.  , 0.  , 0.  , 0.  , 0.75, 0.  ],\n",
       "       [1.  , 0.  , 1.  , 0.  , 0.5 , 0.  , 0.  , 0.  , 2.  , 0.  ],\n",
       "       [2.  , 0.  , 0.  , 0.  , 0.  , 1.  , 0.25, 0.  , 1.25, 0.  ],\n",
       "       [1.  , 0.  , 0.  , 1.5 , 0.  , 1.  , 1.  , 0.  , 0.  , 0.  ],\n",
       "       [1.  , 0.  , 0.  , 2.  , 0.  , 0.  , 0.75, 0.  , 0.75, 0.  ],\n",
       "       [2.  , 0.  , 0.  , 0.5 , 0.  , 1.  , 0.  , 0.  , 1.  , 0.  ],\n",
       "       [1.  , 0.  , 0.  , 0.  , 2.25, 1.  , 0.  , 0.  , 0.25, 0.  ],\n",
       "       [0.  , 0.  , 0.  , 0.  , 4.5 , 0.  , 0.  , 0.  , 0.  , 0.  ],\n",
       "       [0.  , 0.  , 0.  , 0.  , 4.5 , 0.  , 0.  , 0.  , 0.  , 0.  ],\n",
       "       [0.  , 0.  , 0.  , 0.  , 4.5 , 0.  , 0.  , 0.  , 0.  , 0.  ]])"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(X.shape)\n",
    "X[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the shape of (4, 19, 10), there are 4 time images of size 19 rows and 5 columns.  \n",
    "The original time image has 5% increments, so 20 rows, but one is lost when the rolling sum function is applied.  \n",
    "5 columns means in this data sub set that there are 5 unique events."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, if the fit_transform_subblocks's term_weighting = \"tf-idf\" is True then the following transformation will be applied\n",
    "\n",
    "Since the data is 3-dimensional (an array of time images) to apply tf-idf the array is reshaped to 2-dimensional, then after its again reshaped, back to the original 3-dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "# applies tf-idf if pararmeter\n",
    "if term_weighting == \"tf-idf\":\n",
    "\n",
    "    # Set up sizing\n",
    "    num_instance, _, _ = X.shape\n",
    "    dim1, dim2, dim3 = X.shape\n",
    "    X = X.reshape(-1, dim3)\n",
    "\n",
    "    # apply tf-idf\n",
    "    df_vec = np.sum(X > 0, axis=0)\n",
    "    idf_vec = np.log(num_instance / (df_vec + 1e-8))\n",
    "    idf_tile = np.tile(idf_vec, (num_instance * dim2, 1))\n",
    "    idf_matrix = X * idf_tile\n",
    "    X = idf_matrix\n",
    "\n",
    "    # reshape to original dimensions\n",
    "    X = X.reshape(dim1, dim2, dim3)\n",
    "\n",
    "x_train = X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4, 19, 10)"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then once the fit_transform has been applied, the columns, tf-idf information, and other processing parameters are saved to be applied to the test_set with `fit_transform`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = events_df[\"Label\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4, 19, 10)"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(X_new.shape)\n",
    "print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
