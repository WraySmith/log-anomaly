{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering\n",
    "\n",
    "The initial concept and some of the code borrows heavily from [Loglizer](https://github.com/logpai/loglizer)  \n",
    "Input data for the `data_processor.py` file is created by `parse/project_parser.py` and is stored in `parse/project_parsed`\n",
    "\n",
    "This demo will walk you through the process of converting the semi-structured log data created by the `parse` files into `image images` to be fed into a CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import OrderedDict\n",
    "import regex as re\n",
    "from sliding_window_processor import FeatureExtractor, sequence_padder, windower\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The train set is loaded to look at the format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data = pd.read_csv(\"../../project_processed_data/HDFS_100k.log_structured.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now load the y data, and subset the x for easy demonstration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = pd.read_csv(\"../../project_processed_data/anomaly_label.csv\")\n",
    "x_train = input_data[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 9)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, each event is collected into a list for each block id with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_event_ids(data_frame, regex_pattern, column_names):\n",
    "    \"\"\"\n",
    "    turns input data_frame into a 2 columned dataframe\n",
    "    with columns: BlockId, EventSequence\n",
    "    where EventSequence is a list of the events that happened to the block\n",
    "    \"\"\"\n",
    "    data_dict = OrderedDict()\n",
    "    for _, row in data_frame.iterrows():\n",
    "        blk_id_list = re.findall(regex_pattern, row[\"Content\"])\n",
    "        blk_id_set = set(blk_id_list)\n",
    "        for blk_id in blk_id_set:\n",
    "            if blk_id not in data_dict:\n",
    "                data_dict[blk_id] = []\n",
    "            data_dict[blk_id].append(row[\"EventId\"])\n",
    "    data_df = pd.DataFrame(list(data_dict.items()), columns=column_names)\n",
    "    return data_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "re_pat = r\"(blk_-?\\d+)\"\n",
    "col_names = [\"BlockId\", \"EventSequence\"]\n",
    "events_df = collect_event_ids(x_train, re_pat, col_names) # taking a subset for demonstrative purposes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This produced a dataframe with a unique identifier (BlockId) and the list of events in EventSequence  \n",
    "\n",
    "And now join with the y data, so the y data can become split into train and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>BlockId</th>\n",
       "      <th>EventSequence</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>blk_-1608999687919862906</td>\n",
       "      <td>[E5, E22, E5, E5, E11, E11, E9, E9, E11, E9, E...</td>\n",
       "      <td>Normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>blk_7503483334202473044</td>\n",
       "      <td>[E5, E5, E22, E5, E11, E9, E11, E9, E11, E9, E...</td>\n",
       "      <td>Normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>blk_-3544583377289625738</td>\n",
       "      <td>[E5, E22, E5, E5, E11, E9, E11, E9, E11, E9, E...</td>\n",
       "      <td>Anomaly</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>blk_-9073992586687739851</td>\n",
       "      <td>[E5, E22, E5, E5, E11, E9, E11, E9, E11, E9, E...</td>\n",
       "      <td>Normal</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    BlockId  \\\n",
       "0  blk_-1608999687919862906   \n",
       "1   blk_7503483334202473044   \n",
       "2  blk_-3544583377289625738   \n",
       "3  blk_-9073992586687739851   \n",
       "\n",
       "                                       EventSequence    Label  \n",
       "0  [E5, E22, E5, E5, E11, E11, E9, E9, E11, E9, E...   Normal  \n",
       "1  [E5, E5, E22, E5, E11, E9, E11, E9, E11, E9, E...   Normal  \n",
       "2  [E5, E22, E5, E5, E11, E9, E11, E9, E11, E9, E...  Anomaly  \n",
       "3  [E5, E22, E5, E5, E11, E9, E11, E9, E11, E9, E...   Normal  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "events_df = events_df.merge(y, on=\"BlockId\")\n",
    "display(events_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The EventSequence column is then passed to the feature extractor `fit_transform_subblocks()` method  \n",
    "To demonstrate what is happening in the class the code will be dissected and shown here step by step \n",
    "\n",
    "Note: `data_processor.py` also contains `fit_transform()` and `transform()`, these functions are to be used when you don't want to create time images (more on this to come)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "final shape will be  36 10\n",
      "train data shape:  (4, 36, 10)\n"
     ]
    }
   ],
   "source": [
    "# The way the code is called normally\n",
    "events_values = events_df[\"EventSequence\"].values\n",
    "fe = FeatureExtractor()\n",
    "subblocks_train = fe.fit_transform(events_values, term_weighting=\"tf-idf\", length_percentile=100, window_size=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we will define the parameters of the method call"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "window_size = 10\n",
    "term_weighting = \"tf-idf\"\n",
    "length_percentile=100\n",
    "\n",
    "X_seq = events_values\n",
    "\n",
    "max_seq_length = max(np.array(list(map(len, X_seq))))\n",
    "\n",
    "num_rows = max_seq_length - window_size + 1\n",
    "\n",
    "unique_events = set()\n",
    "for i in X_seq:\n",
    "    unique_events.update(i)\n",
    "events = unique_events"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will be turning each event sequence into a time image.  \n",
    "This is done by applying a sliding window to the event sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loop over each sequence to create the time image\n",
    "time_images = []\n",
    "for block in X_seq:\n",
    "    padded_block = sequence_padder(block, max_seq_length)\n",
    "    time_image = windower(padded_block, window_size)\n",
    "    time_image_counts = []\n",
    "    for time_row in time_image:\n",
    "        row_count = Counter(time_row)\n",
    "        time_image_counts.append(row_count)\n",
    "\n",
    "    time_image_df = pd.DataFrame(time_image_counts, columns=events)\n",
    "    time_image_df = time_image_df.reindex(sorted(time_image_df.columns), axis=1)\n",
    "    time_image_df = time_image_df.fillna(0)\n",
    "    time_image_np = time_image_df.to_numpy()\n",
    "\n",
    "    # resize if too large\n",
    "    if len(time_image_np) > num_rows:\n",
    "\n",
    "        time_image_np = resize_time_image(\n",
    "            time_image_np, (num_rows, len(self.events)),\n",
    "        )\n",
    "\n",
    "    time_images.append(time_image_np)\n",
    "\n",
    "# stack all the blocks\n",
    "X = np.stack(time_images)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the first time image from the `X` numpy array:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4, 36, 10)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[3., 0., 0., 1., 0., 0., 0., 3., 0., 3.],\n",
       "       [3., 0., 0., 1., 0., 1., 0., 2., 0., 3.],\n",
       "       [3., 0., 0., 0., 0., 2., 0., 2., 0., 3.],\n",
       "       [3., 0., 0., 0., 0., 3., 0., 1., 0., 3.],\n",
       "       [3., 0., 0., 0., 0., 3., 0., 0., 1., 3.],\n",
       "       [2., 0., 0., 0., 0., 3., 0., 1., 1., 3.],\n",
       "       [1., 1., 0., 0., 0., 3., 0., 1., 1., 3.],\n",
       "       [1., 1., 0., 0., 0., 3., 0., 1., 2., 2.],\n",
       "       [1., 1., 0., 0., 0., 3., 0., 2., 2., 1.],\n",
       "       [0., 1., 1., 0., 0., 3., 0., 2., 2., 1.],\n",
       "       [0., 1., 1., 0., 1., 3., 0., 2., 2., 0.],\n",
       "       [0., 1., 1., 0., 1., 3., 0., 2., 2., 0.],\n",
       "       [0., 1., 1., 0., 1., 3., 0., 2., 2., 0.],\n",
       "       [0., 1., 1., 0., 1., 2., 1., 2., 2., 0.],\n",
       "       [0., 1., 1., 0., 2., 2., 1., 2., 1., 0.],\n",
       "       [0., 1., 1., 0., 2., 2., 1., 1., 2., 0.],\n",
       "       [0., 0., 1., 0., 2., 2., 1., 1., 3., 0.],\n",
       "       [0., 0., 1., 0., 2., 2., 1., 2., 2., 0.],\n",
       "       [0., 0., 1., 0., 2., 2., 1., 2., 2., 0.],\n",
       "       [0., 1., 0., 0., 2., 2., 1., 2., 2., 0.],\n",
       "       [0., 1., 1., 0., 1., 2., 1., 2., 2., 0.],\n",
       "       [0., 1., 1., 0., 1., 2., 1., 2., 2., 0.],\n",
       "       [0., 1., 1., 0., 1., 2., 1., 2., 2., 0.],\n",
       "       [0., 1., 1., 0., 1., 2., 0., 3., 2., 0.],\n",
       "       [0., 1., 1., 0., 0., 2., 0., 3., 3., 0.],\n",
       "       [0., 1., 1., 0., 0., 2., 0., 4., 2., 0.],\n",
       "       [0., 2., 1., 0., 0., 2., 0., 4., 1., 0.],\n",
       "       [0., 2., 1., 0., 0., 2., 1., 3., 1., 0.],\n",
       "       [0., 2., 1., 0., 0., 2., 2., 2., 1., 0.],\n",
       "       [0., 1., 1., 0., 0., 2., 3., 2., 1., 0.],\n",
       "       [0., 1., 0., 0., 0., 2., 4., 2., 1., 0.],\n",
       "       [0., 1., 0., 0., 0., 1., 5., 2., 1., 0.],\n",
       "       [0., 1., 0., 0., 0., 0., 6., 2., 1., 0.],\n",
       "       [0., 1., 0., 0., 0., 0., 7., 1., 1., 0.],\n",
       "       [0., 1., 0., 0., 0., 0., 8., 1., 0., 0.],\n",
       "       [0., 1., 0., 0., 0., 0., 9., 0., 0., 0.]])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(X.shape)\n",
    "X[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the shape of (4, 36, 10), there are 4 time images of size 36 rows and 10 columns.  \n",
    "10 columns means in this data sub set that there are 10 unique events."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, if the fit_transform's term_weighting = \"tf-idf\" is True then the following transformation will be applied\n",
    "\n",
    "Since the data is 3-dimensional (an array of time images) to apply tf-idf the array is reshaped to 2-dimensional, then after its again reshaped, back to the original 3-dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# applies tf-idf if pararmeter\n",
    "if term_weighting == \"tf-idf\":\n",
    "\n",
    "    # Set up sizing\n",
    "    num_instance, _, _ = X.shape\n",
    "    dim1, dim2, dim3 = X.shape\n",
    "    X = X.reshape(-1, dim3)\n",
    "\n",
    "    # apply tf-idf\n",
    "    df_vec = np.sum(X > 0, axis=0)\n",
    "    idf_vec = np.log(num_instance / (df_vec + 1e-8))\n",
    "    idf_tile = np.tile(idf_vec, (num_instance * dim2, 1))\n",
    "    idf_matrix = X * idf_tile\n",
    "    X = idf_matrix\n",
    "\n",
    "    # reshape to original dimensions\n",
    "    X = X.reshape(dim1, dim2, dim3)\n",
    "\n",
    "x_train = X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4, 36, 10)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then once the fit_transform has been applied, the columns, tf-idf information, and other processing parameters are saved to be applied to the test_set with `fit_transform`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = events_df[\"Label\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4, 36, 10)\n",
      "(4,)\n"
     ]
    }
   ],
   "source": [
    "print(x_train.shape)\n",
    "print(y_train.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So now the train data set is complete.  \n",
    "To create the test data set, the same process is run after, but with `transform()` instead of `fit_transform()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
