{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "sublime-associate",
   "metadata": {},
   "source": [
    "# Raw HDFS Data Review and Train/Test Split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "recovered-sensitivity",
   "metadata": {},
   "source": [
    "The HDFS data used in this project is provided by the [Loghub collection](https://github.com/logpai/loghub):\n",
    "- Shilin He, Jieming Zhu, Pinjia He, Michael R. Lyu. [Loghub: A Large Collection of System Log Datasets towards Automated Log Analytics](https://arxiv.org/abs/2008.06448). *Arxiv*, 2020."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fallen-examination",
   "metadata": {},
   "source": [
    "The HDFS data are provided at: https://github.com/logpai/loghub/tree/master/HDFS\n",
    "\n",
    "The downloaded file HDFS_1.tar.gz which provides the HDFS.log data used in this project is from: https://zenodo.org/record/3227177#.YHH_tOhKhPY\n",
    "\n",
    "The downloaded HDFs_1.tar.gz also provides a file `anomaly_label.csv` which provides a label whether each HDFS block in the `HDFS.log` file in normal or anomalous."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "future-beaver",
   "metadata": {},
   "source": [
    "Additional details of the `HDFS.log` file are provided in the paper:\n",
    "- Wei Xu, Ling Huang, Armando Fox, David Patterson, Michael Jordan. [Detecting Large-Scale System Problems by Mining Console Logs](https://people.eecs.berkeley.edu/~jordan/papers/xu-etal-sosp09.pdf), in Proc. of the 22nd ACM Symposium on Operating Systems Principles (SOSP), 2009. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "norwegian-holly",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import logging"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "complete-australia",
   "metadata": {},
   "source": [
    "## Explore raw data in HDFS.log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "parallel-democrat",
   "metadata": {},
   "outputs": [],
   "source": [
    "# count rows\n",
    "with open('project_raw/HDFS.log', \"r\") as file:\n",
    "    totaln=0\n",
    "    for line in file:\n",
    "        totaln += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "polish-column",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are a total of 11175629 lines\n"
     ]
    }
   ],
   "source": [
    "print('There are a total of {} lines'.format(totaln))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fancy-match",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Take a quick look at the data:\n",
    "   \n",
    "data = []\n",
    "with open('project_raw/HDFS.log', \"r\") as file:\n",
    "    n=0\n",
    "    for line in file:\n",
    "        data.append(line)\n",
    "        if n <200:\n",
    "            n += 1\n",
    "        else: break "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "surprised-guard",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "commercial-consortium",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>081109 203518 143 INFO dfs.DataNode$DataXceive...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>081109 203518 35 INFO dfs.FSNamesystem: BLOCK*...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>081109 203519 143 INFO dfs.DataNode$DataXceive...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>081109 203519 145 INFO dfs.DataNode$DataXceive...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>081109 203519 145 INFO dfs.DataNode$PacketResp...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   0\n",
       "0  081109 203518 143 INFO dfs.DataNode$DataXceive...\n",
       "1  081109 203518 35 INFO dfs.FSNamesystem: BLOCK*...\n",
       "2  081109 203519 143 INFO dfs.DataNode$DataXceive...\n",
       "3  081109 203519 145 INFO dfs.DataNode$DataXceive...\n",
       "4  081109 203519 145 INFO dfs.DataNode$PacketResp..."
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "imposed-ministry",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([['081109 203527 154 INFO dfs.DataNode$DataXceiver: 10.251.197.226:50010 Served block blk_-3544583377289625738 to /10.251.203.4\\n'],\n",
       "       ['081109 203527 154 INFO dfs.DataNode$DataXceiver: 10.251.215.16:50010 Served block blk_-1608999687919862906 to /10.250.19.227\\n'],\n",
       "       ['081109 203527 155 INFO dfs.DataNode$DataXceiver: 10.250.11.100:50010 Served block blk_-3544583377289625738 to /10.250.19.227\\n'],\n",
       "       ['081109 203527 155 INFO dfs.DataNode$DataXceiver: 10.251.197.226:50010 Served block blk_-3544583377289625738 to /10.251.215.16\\n'],\n",
       "       ['081109 203527 156 INFO dfs.DataNode$DataXceiver: 10.250.11.100:50010 Served block blk_-3544583377289625738 to /10.251.65.203\\n'],\n",
       "       ['081109 203527 156 INFO dfs.DataNode$DataXceiver: 10.251.197.226:50010 Served block blk_-3544583377289625738 to /10.250.17.177\\n'],\n",
       "       ['081109 203527 157 INFO dfs.DataNode$DataXceiver: 10.250.11.100:50010 Served block blk_-3544583377289625738 to /10.251.66.63\\n'],\n",
       "       ['081109 203527 157 INFO dfs.DataNode$DataXceiver: 10.251.197.226:50010 Served block blk_-3544583377289625738 to /10.251.201.204\\n'],\n",
       "       ['081109 203527 158 INFO dfs.DataNode$DataXceiver: 10.250.11.100:50010 Served block blk_-3544583377289625738 to /10.251.126.5\\n'],\n",
       "       ['081109 203527 158 INFO dfs.DataNode$DataXceiver: 10.251.197.226:50010 Served block blk_-3544583377289625738 to /10.251.126.83\\n'],\n",
       "       ['081109 203527 159 INFO dfs.DataNode$DataXceiver: 10.251.197.226:50010 Served block blk_-3544583377289625738 to /10.251.126.255\\n'],\n",
       "       ['081109 203527 160 INFO dfs.DataNode$DataXceiver: 10.251.197.226:50010 Served block blk_-3544583377289625738 to /10.251.125.237\\n'],\n",
       "       ['081109 203527 161 INFO dfs.DataNode$DataXceiver: 10.251.197.226:50010 Served block blk_-3544583377289625738 to /10.251.203.246\\n'],\n",
       "       ['081109 203527 162 INFO dfs.DataNode$DataXceiver: 10.251.197.226:50010 Served block blk_-3544583377289625738 to /10.251.111.209\\n'],\n",
       "       ['081109 203527 163 INFO dfs.DataNode$DataXceiver: 10.251.197.226:50010 Served block blk_-3544583377289625738 to /10.251.203.166\\n'],\n",
       "       ['081109 203527 164 INFO dfs.DataNode$DataXceiver: 10.251.197.226:50010 Served block blk_-3544583377289625738 to /10.251.31.160\\n'],\n",
       "       ['081109 203527 19 INFO dfs.DataNode: 10.251.107.19:50010 Starting thread to transfer block blk_-1608999687919862906 to 10.251.31.5:50010, 10.251.71.240:50010\\n'],\n",
       "       ['081109 203527 19 INFO dfs.FSNamesystem: BLOCK* ask 10.251.107.19:50010 to replicate blk_-1608999687919862906 to datanode(s) 10.251.31.5:50010 10.251.71.240:50010\\n'],\n",
       "       ['081109 203528 145 INFO dfs.DataNode$DataXceiver: Received block blk_-1608999687919862906 src: /10.251.31.5:53020 dest: /10.251.31.5:50010 of size 91178\\n'],\n",
       "       ['081109 203528 148 INFO dfs.DataNode$DataXceiver: 10.251.74.79:50010 Served block blk_-1608999687919862906 to /10.251.127.243\\n']],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.iloc[100:120].values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "welcome-grass",
   "metadata": {},
   "source": [
    "The data are unstructed log files. After reviewing several anomaly detection papers, Drain was identified as the most accurate parser as part of the evaluation and Logparser provides the implementation of Drain discussed in:\n",
    "- [**ICWS'17**] [Drain: An Online Log Parsing Approach with Fixed Depth Tree](https://jiemingzhu.github.io/pub/pjhe_icws2017.pdf), by Pinjia He, Jieming Zhu, Zibin Zheng, and Michael R. Lyu."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "colonial-twelve",
   "metadata": {},
   "source": [
    "An implementation of the Drain log parser is available through the [Logparser toolkit](https://github.com/logpai/logparser). The Logparser toolkit provides multiple automated log parsing methods to create structured logs (also referred to as message template extraction). Logparser was created as part of an evaluation of various parsers:\n",
    "- [**ICSE'19**] Jieming Zhu, Shilin He, Jinyang Liu, Pinjia He, Qi Xie, Zibin Zheng, Michael R. Lyu. [Tools and Benchmarks for Automated Log Parsing](https://arxiv.org/pdf/1811.03509.pdf). *International Conference on Software Engineering (ICSE)*, 2019.\n",
    "+ [**DSN'16**] Pinjia He, Jieming Zhu, Shilin He, Jian Li, Michael R. Lyu. [An Evaluation Study on Log Parsing and Its Use in Log Mining](https://jiemingzhu.github.io/pub/pjhe_dsn2016.pdf). *IEEE/IFIP International Conference on Dependable Systems and Networks (DSN)*, 2016."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "indirect-senate",
   "metadata": {},
   "source": [
    "However, Drain as provided in the Logparser package is implemented in Python 2.7. Documentation for Logparser can also be found [here](https://logparser.readthedocs.io/en/latest/README.html). Accordingly, running Drain is completed in a Python 2.7 environment and not completed in this notebook. The `project_parser.py` script uses Drain to parse both the `HDFS.log` and `HDFS_train.log` files described below."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "minute-memorabilia",
   "metadata": {},
   "source": [
    "## Training and Testing .log Files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bored-treasurer",
   "metadata": {},
   "source": [
    "Split the data into a training and testing set based on 80/20 split. Using the ordered data (no shuffling or random selection) as they are based on a log series history and we want to use the last 20% for test and we want to maintain the order of data.\n",
    "\n",
    "Note that we're only creating a train file as we'll do the following procedure with the Drain log parser:\n",
    "- Run Drain on the train file - this will create log templates only based on the training set\n",
    "- Run Drain on the complete file - this will create the same log templates from the training set but will create any potential new templates only seen in the testing data, this may also result in modification of the original training data set templates (Drain updates teamplates as it learns new patterns) but these updated templates will not be copied back to the training set as they would not have been seen at that point. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "sized-sentence",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8940503"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_idx = int(totaln*.8)\n",
    "train_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "subsequent-lawsuit",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the training lines only\n",
    "train_data = []\n",
    "with open('project_raw/HDFS.log', \"r\") as file:\n",
    "    n=0\n",
    "    for line in file:\n",
    "        if n < train_idx:\n",
    "            train_data.append(line)\n",
    "            n += 1\n",
    "        else: break "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "stupid-scene",
   "metadata": {},
   "outputs": [],
   "source": [
    "# write to the training file `HDFS_train.log`\n",
    "with open('project_raw/HDFS_train.log', 'x') as file:\n",
    "    for i in train_data:\n",
    "        file.write(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "productive-divorce",
   "metadata": {},
   "source": [
    "## Drain Output Files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "forced-pepper",
   "metadata": {},
   "source": [
    "The `project_parser.py` script is run on both the `HDFS.log` and `HDFS_train.log` files as discussed above. The outputs created which are then used for feature extraction are:\n",
    "\n",
    "- `HDFS_train.log_templates.csv`\n",
    "- `HDFS_train.log_structured.csv`\n",
    "- `HDFS.log_templates.csv`\n",
    "- `HDFS.log_structured.csv`\n",
    "\n",
    "`HDFS.log_templates.csv` can be used directly from the testing feature extraction but to create a testing file with only the structured log data, the training logs from `HDFS.log_structured.csv` will be removed and the testing only structured log file will be saved as `HDFS_test.log_structured.csv`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "occasional-couple",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_parsed = pd.read_csv('project_parsed/HDFS.log_structured.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "sized-motel",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_parsed = all_parsed.iloc[train_idx:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "least-nitrogen",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_parsed.to_csv('project_parsed/HDFS_test.log_structured.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "increasing-lancaster",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
